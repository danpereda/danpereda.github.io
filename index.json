[{"authors":["admin"],"categories":null,"content":"I am a first year (cotutelle) PhD student in Engineering Sciences with specialization in Math Modeling at Universidad de Chile, √âcole centrale de Lille and Inria.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://danpereda.github.io/author/daniel-pereda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/daniel-pereda/","section":"authors","summary":"I am a first year (cotutelle) PhD student in Engineering Sciences with specialization in Math Modeling at Universidad de Chile, √âcole centrale de Lille and Inria.","tags":null,"title":"Daniel Pereda","type":"authors"},{"authors":[],"categories":[],"content":"","date":1596178628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596178628,"objectID":"111b6d56e56f3facecaa145646e68d50","permalink":"https://danpereda.github.io/project/icovidchile/","publishdate":"2020-07-31T02:57:08-04:00","relpermalink":"/project/icovidchile/","section":"project","summary":"","tags":[],"title":"ICOVIDChile","type":"project"},{"authors":[],"categories":[],"content":"Here is what I\u0026rsquo;ve learned from the WorkShop Doing Scientific Machine Learning (SciML) With Julia from Chris Rackauckas. There is also an MIT course and Workshop exercises (with solutions) by the same author about this subject that I\u0026rsquo;ve been checking out and strongly recomend.\nTable of Contents  Modeling with Differential Equations  Differential Equations Stochastic Differential Equations Delayed Differential Equations Callbacks   Automated model discovery via universal differential equations  Parameter inference on differential equations  Local and global optimization Bayesian optimization   Neural Ordinary Differential Equations with sciml_train  Solving for the Lokta - Volterra model with few data. Universal ODEs learn and extrapolate other dynamical behaviors Transforming a neural network fit into equations in sparsified from using SInDy     Solving differential equations with neural networks (physics-informed neural networks)  Toy example Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation    Modeling with Differential Equations Differential Equations First we will see how to define a differential Equation on Julia, for this we will use the Latka Volterra equation that modelates a population of rabbits and wolves.\n$$ \\dfrac{dx}{dt} = \\alpha x - \\beta xy$$ $$ \\dfrac{dy}{dt} = \\delta xy - \\gamma y$$\nSomething that may be silly but I find really nice is that you can write special caracters like üê∞, üê∫, Œ±, Œ≤, Œ≥ and Œ¥.\nWe just need to charge the package DifferentialEquations.jl and write our differential equation as a function.\nusing DifferentialEquations\rfunction lakta_volterra!(du,u,p,t)\rüê∞,üê∫ = u\rŒ±,Œ≤,Œ≥,Œ¥ = p\rdu[1] = düê∞ = Œ±*üê∞ - Œ≤*üê∞*üê∫\rdu[2] = düê∫ = Œ≥*üê∞*üê∫ - Œ¥*üê∫\rend\ru‚ÇÄ = [1.0,1.0]\rtspan = (0.0, 10.0)\rp = [1.5,1.0,3.0,1.0]\rprob = ODEProblem(lakta_volterra!,u‚ÇÄ,tspan,p)\rsol = solve(prob)\r \r\rLakta- Volterra Solution\r\r\r\r\rRabbit vs Wolves\r\r\rEasy optimizations can be made, we can choose a better solver for the problem, stop saving everystep, etc.\nusing Sundials # Charge CVODE_BDF() solver\rsol = solve(prob, CVODE_BDF(), save_everystep=false, abstol=1e-8, reltol=1e-8)\r We can also change the parameters by using the remake function.\nremake(prob, p =[1.2,0.8,2.5,0.8])\r Stochastic Differential Equation Now we have a multiplicative noise, given by the terms $\\sigma_i x_i dW_i$ where $dW_i$ is a random number whos standard deviation is $dt$. $$ dx = (\\alpha x - \\beta xy)dt + \\sigma_1 x dW_1 $$ $$ dy = (\\delta xy - \\gamma y)dt + \\sigma_2 y dW_2$$\nIn julia we just need create the multiplicative noise function and added to the previous problem by using SDEProblem instead of ODEProblem.\nfunction multiplicative_noise!(du,u,p,t)\rüê∞,üê∫ = u\rdu[1] = 0.3*üê∞\rdu[2] = 0.3*üê∫\rend\rprob = SDEProblem(lakta_volterra!,multiplicative_noise!,u‚ÇÄ,tspan,p)\rsol = solve(prob)\r Solving only once would not be the best, since we have a randomness, thus we made use of another set of functions already implemented in DifferentialEquations.jl called Ensemble. Firstly we ensemble the problem, secondly we solve for a given number of trajectories (aditonal parameters like EnsembleThreads can be written in order to parelalize the problem and get aditional performance) and finally we do a summary of what happened.\nensembleprob = EnsembleProblem(prob)\rsol = solve(ensembleprob, SOSRI(), EnsembleThreads(), trajectories=1000)\rsumm = EnsembleSummary(sol)\r \r\rSimple Summary\r\r\r\rClick here to see more about Ensemble\nDelayed Differential Equations The amount of growth happening at time $t$ is not due to the amount of rabbits at time $t$\n\r\rDelayed Differential Equation\r\r\rPopulation Control Example, whenever the amount of wolves is equal to $3$ then we are allow to kill one. The key feature to do this Callbacks. So whenever a condition happens, then it affects the dynamics.\nüî•üê∫_condition(u,t,integrator) = u[2] - 3\rüî•üê∫_affect!(integrator) = integrator.u[2] -=1\rüî•üê∫_cb = ContinuousCallback(üî•üê∫_condition,üî•üê∫_affect!)\rsol = solve(prob, callback = üî•üê∫_cb)\r \r\rPopulation Control\r\r\rOf course this is not the most realistic model, since we don\u0026rsquo;t instanstly kill a wolf each time.\nAutomated model discovery via universal differential equations Parameter inference on differential equations Our goal will be to find parameters that make the Lotka-Volterra solution the one we had on the first part, so we define our loss as the squared distance from our the real solution dataset = Array(sol) with parameters $p$ given by $\\alpha = 1.5$, $\\beta = 1.0$, $\\gamma = 3.0$ and $\\delta = 1.0$. Note that when using sciml_train, the first return is the loss value, and the other returns are sent to the callback for monitoring convergence.\nfunction loss(p)\rtmp_prob = remake(prob, p = p)\rtmp_sol = solve(tmp_prob, saveat = 0.1)\rsum(abs2, Array(tmp_sol) - dataset), tmp_sol\rend\r Lastly, we use the sciml_train function to train the parameters using BFGS to arrive at parameters which optimize for our goal.\nusing DiffEqFlux\rusing Optim\rpinit = [1.2,0.8,2.5,0.8]\rres = DiffEqFlux.sciml_train(loss, pinit, BFGS(), maxiters = 100)\rp_final = res.minimizer\r sciml_train allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation.\nusing Flux\rfunction plot_callback(p,l,tmp_sol)\rtmp_prob = remake(prob, p = p)\rtmp_sol = solve(tmp_prob, saveat = 0.1)\rfig = plot(tmp_sol)\rscatter!(fig, sol.t,dataset')\rdisplay(fig)\rfalse\rend\r Let\u0026rsquo;s optimize the model and get a nice animation of what is happening in each iteration:\nres = DiffEqFlux.sciml_train(loss, pinit, BFGS(), cb = plot_callback, maxiters=300)\rp_final = res.minimizer\r In just $1.745$ seconds and $8658251$ allocations: 361.20 MiB (counting plots) we get a loss function of $2.401364 \\times 10^{-23}$ and parameters: $$\\alpha = 1.5000000000009583 \\approx 1.5$$ $$\\beta = 1.0000000000008002 \\approx 1.0$$ $$\\gamma = 3.0000000000005405 \\approx 3.0$$ $$\\delta = 0.9999999999995174 \\approx 1.0$$\n\rClick here to see more\nNotice that the election of BFGS makes us converge quickier than using ADAM (try this yourself). Usually ADAM its pretty good for the first iterations to get local optima but then its better to change to BFGS to do the final steps. Otherwise we can use BlackBoxOptim to get global optima algorithms.\nusing BlackBoxOptim\rres = DiffEqFlux.sciml_train(loss, pinit, DiffEqFlux.BBO(), cb = plot_callback,\rlower_bounds= 0.5ones(4),\rupper_bounds=4.0ones(4) )\r After $48690$ steps we get best candidate found: $[1.5, 1.0, 3.0, 1.0]$\nBayesian Inference In this section we will use Turing.jl together with the documentation Click here to see more.\nMost of the scientific community deals with the basic problem of trying to mathematically model the reality around them and this often involves dynamical systems. The general trend to model these complex dynamical systems is through the use of differential equations. Differential equation models often have non-measurable parameters. The popular ‚Äúforward-problem‚Äù of simulation consists of solving the differential equations for a given set of parameters, the ‚Äúinverse problem‚Äù to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters. Bayesian inference provides a robust approach to parameter estimation with quantified uncertainty.\nFirst we set up all the packages that will be use together with a fix seed for reproducibility of the results.\nusing Turing, Distributions, DataFrames, DifferentialEquations, DiffEqSensitivity\r# Import MCMCChain, Plots, and StatsPlots for visualizations and diagnostics.\rusing MCMCChains, Plots, StatsPlots\r# Set a seed for reproducibility.\rusing Random\rRandom.seed!(12);\r We will keep using the same Lotka-Volerra equation and we‚Äôll generate the data to use for the parameter estimation from simulation.\nodedata = Array(solve(prob,Tsit5(),saveat=0.1))\r Turing and DifferentialEquations are completely composable and you can write of the differential equation inside a Turing @model and it will just work.\nWe can rewrite the Lotka Volterra parameter estimation problem with a Turing @model interface as below\nTuring.setadbackend(:forward_diff) #Small Model\r@model function fitlv(data)\rœÉ ~ InverseGamma(2, 3)\rŒ± ~ truncated(Normal(1.5,0.5),0.5,2.5)\rŒ≤ ~ truncated(Normal(1.2,0.5),0,2)\rŒ≥ ~ truncated(Normal(3.0,0.5),1,4)\rŒ¥ ~ truncated(Normal(1.0,0.5),0,2)\rp = [Œ±,Œ≤,Œ≥,Œ¥]\rprob = ODEProblem(lokta_volterra!,u‚ÇÄ,tspan,p)\rpredicted = solve(prob,Tsit5(),saveat=0.1)\rfor i = 1:length(predicted)\rdata[:,i] ~ MvNormal(predicted[i], œÉ) #Maximum Likehood Estimation\rend\rend\rmodel = fitlv(odedata)\rchain = sample(model, NUTS(.65),10000)\r We just give our prior distribution and solve the dynamics to calcule our predictions and then compare it with the data in a maximum likehood estimation.\nNeural Ordinary Differential Equations with sciml_train First, lets generate the data\nusing DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\ru0 = Float32[2.0; 0.0]\rdatasize = 30\rtspan = (0.0f0, 1.5f0)\rtsteps = range(tspan[1], tspan[2], length = datasize)\rfunction trueODEfunc(du, u, p, t)\rtrue_A = [-0.1 2.0; -2.0 -0.1]\rdu .= ((u.^3)'true_A)'\rend\rprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\rode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r Second, we create a neural network that represents some idea we know about the system.\ndudt2 = FastChain((x, p) -\u0026gt; x.^3,\rFastDense(2, 50, tanh),\rFastDense(50, 2))\r What the neural network is just a mathematical function with the right parameters, what the code is doing is just writting:\n$ Wx^3 \\rightarrow Wx^3 + b \\rightarrow tanh(Wx^3+b)$ $\\rightarrow W_2 tanh(Wx^3+b) \\rightarrow W_2 tanh(Wx^3+b) + b_2 $\nNote: For large neural networks its recommended to use Flux instead of DiffEqFlux.\nNow we can write the ODEProblem and solve it.\nneural_ode_f(u,p,t) = dudt2(u,p)\rpinit = initial_params(dudt2)\rprob = ODEProblem(neural_ode_f,u0,(0.0f0,1.5f0),pinit)\rsol = solve(prob, saveat = tsteps)\r \r\rNeural ODE\r\r\rAs we see the initial guess is not good, since we just try it to approximate by a random ODE. Then what we need to do its find the right parameters that describe the neural network such that matches the ODE well enough. Therefore we can do it as before:\nfunction loss(p)\rtmp_prob = remake(prob,p=p)\rtmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)\rsum(abs2, Array(tmp_sol) - ode_data)\rend\rfunction neuralode_callback(p,l)\rtmp_prob = remake(prob,p=p)\rtmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)\rfig = plot(tmp_sol)\rscatter!(fig,tsteps,ode_data') display(fig)\rfalse\rend\rDiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),\rcb = neuralode_callback,\rmaxiters = 500)\r We get a loss value of $0.0678$. This can be optimize by using ADAM and then BFGS\nres = DiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),\rcb = neuralode_callback,\rmaxiters = 100)\rDiffEqFlux.sciml_train(loss, res.minimizer, BFGS(initial_stepnorm=0.01),\rmaxiters = 100,\rcb = neuralode_callback)\r Now we get a loss value of $ 1.591519 \\times 10^{-3}$.\nWe can see that most of the computational time is on the gradients. For instance Zygote.jl and Turing.jl take control over which algorithm is used in order to optimize performance, anyways we can always choose which one we want see DifferentialEquations.jl documentation on Sensitivity Algorithms for this matter.\n\rClick here to check more about Neural ODE on the SciML ecosystem.\nUniversal ODEs learn and extrapolate other dynamical behaviors Truth equation:\n$$ \\dot{x} = \\alpha x - \\beta xy$$ $$ \\dot{y} = \\gamma xy - \\delta y$$\nPartially-known neural embedded equations\n$$ \\dot{x} = \\alpha x - U_1(x,y)$$ $$ \\dot{y} = -\\delta y + U_2(x,y)$$\nAutomatically recover the long-term behaviour from less than half of a period in a cyclick time series!\nTurn neural networks back intro equations with SInDy.\nLet\u0026rsquo;s define the experimental parameter for the Lokta - Volterra equation.\ntspan = (0.0f0,3.0f0)\ru0 = Float32[0.44249296,4.6280594]\rp_ = Float32[1.3, 0.9, 0.8, 1.8]\rprob = ODEProblem(lotka, u0,tspan, p_)\rsolution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\r \r\rFew data\r\r\rThen we add noise to the data so that we do not overfit.\nX = Array(solution)\rX‚Çô = X + Float32(1e-3)*randn(eltype(X), size(X))\r Define the neueral network which learns $L(x, y, y(t-\\tau))$. Actually, we do not care about overfitting right now, since we want to extract the derivative information without numerical differentiation.\nL = FastChain(FastDense(2, 32, tanh),FastDense(32, 32, tanh), FastDense(32, 2))\rp = initial_params(L)\r Let\u0026rsquo;s define now the neural network given by:\n$$ \\dot{x} = \\alpha x - U_1(x,y)$$ $$ \\dot{y} = -\\delta y + U_2(x,y)$$\nfunction dudt_(u, p,t)\rx, y = u\rz = L(u,p)\r[p_[1]*x + z[1],\r-p_[4]*y + z[2]]\rend\r So then when we solve\nprob_nn = ODEProblem(dudt_,u0, tspan, p)\rsol_nn = concrete_solve(prob_nn, Tsit5(), u0, p, saveat = solution.t)\r The thick curves represent the real solution, as we see, we get a decent predictor for only the first second, afterwards the prediction for $u_1(t)$ its pretty bad, while the prediction for $u_2(t)$ it\u0026rsquo;s ok.\n\r\rBad predictor\r\r\rLet\u0026rsquo;s improve now. For this we will do as before, we perfom a prediction and then compute a loss function on the prediction to check how well are we fitting the data. Finally we create a Callback that saves the losses each $50$ iterations.\nfunction predict(Œ∏)\rArray(concrete_solve(prob_nn, Vern7(), u0, Œ∏, saveat = solution.t,\rabstol=1e-6, reltol=1e-6,\rsensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))\rend\rfunction loss(Œ∏)\rpred = predict(Œ∏)\rsum(abs2, X‚Çô .- pred), pred end\rconst losses = []\rcallback(Œ∏,l,pred) = begin\rpush!(losses, l)\rif length(losses)%50==0\rprintln(\u0026quot;Current loss after $(length(losses)) iterations: $(losses[end])\u0026quot;)\rend\rfalse\rend\r First train with ADAM for better convergence adn then train with BFGS\nres1 = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 200)\rres2 = DiffEqFlux.sciml_train(loss, res1.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000)\r \r\rlosses\r\r\rFinal training loss after $482$ iterations is $2.74 \\times 10^{-4}$ and the approximation fits the real solution really well.\n\r\rReal solution vs Approximation\r\r\rNotice that we purposely made the real solution curve thicker so that its easier to see, otherwise both curves are superposed.\nWe can also check the derivatives.\nDX = Array(solution(solution.t, Val{1}))\rprob_nn2 = ODEProblem(dudt_,u0, tspan, res2.minimizer)\r_sol = solve(prob_nn2, Tsit5())\rDX_ = Array(_sol(solution.t, Val{1}))\r \r\rDerivatives\r\r\rFinally, we know that the real functions are $\\beta xy$ and $\\gamma xy$. Lets check the error plot.\n# Ideal data\rLÃÑ = [-p_[2]*(X[1,:].*X[2,:])';p_[3]*(X[1,:].*X[2,:])']\r# Neural network guess\rLÃÇ = L(X‚Çô,res2.minimizer)\r \r\rReal solution vs Approximation\r\r\rTransforming a neural network fit into equations in sparsified from using SInDy Now we want to use this nice fit and transformed back into equations. For this porpuse we\u0026rsquo;ll use ModelingToolkit.jl.\nWe will let the model know that we have $2$ variables and then create a basis that can approximate this functions by linear combinations of $sin(u_1)$ , $cos(u_1)$, $sin(u_2)$, $cos(u_2)$, $constant$, $u_1(t)^k$, $u_2(t)^k$ and $u_1(t)^k * u_2(t)^{5-k}$ with $k = 1\u0026hellip;5$\n@variables u[1:2]\r# Lots of polynomials\rpolys = Operation[1]\rfor i ‚àà 1:5\rpush!(polys, u[1]^i)\rpush!(polys, u[2]^i)\rfor j ‚àà i:5\rif i != j\rpush!(polys, (u[1]^i)*(u[2]^j))\rpush!(polys, u[2]^i*u[1]^i)\rend\rend\rend\r# And some other stuff\rh = [cos.(u)...; sin.(u)...; polys...]\rbasis = Basis(h, u)\r Now we will create an optimizer for the SINDY problem Check more about Sparse Identification of Nonlinear Dynamics\n# Create an optimizer for the SINDY problem\ropt = SR3()\r# Create the thresholds which should be used in the search process\rŒª = exp10.(-7:0.1:3)\r# Target function to choose the results from; x = L0 of coefficients and L2-Error of the model\rf_target(x, w) = iszero(x[1]) ? Inf : norm(w.*x, 2)\r Let\u0026rsquo;s see what happens if we want to use pure SINDY, meaning we have no pior information, only the data generated by our neural network,i.e, we took the values of the differential equation through the time series, we run it on the neural network giving us the output $\\hat{L}$ and the $X$ is the input of the neural network, then we make SINDY transform this data into a system of equations using the basis.\nŒ® = SInDy(X‚Çô[:, :], DX[:, :], basis, Œª, opt = opt, maxiter = 10000,\rf_target = f_target)\r As a result we get : $$du_1 = \\beta sin(u_1) + \\alpha cos(u_2) + \\gamma u_1^2$$ $$du_2 = \\delta u_2 $$\ni.e, we failed, but remember that we didn\u0026rsquo;t use any pior information.\nTest on ideal derivative data for unknown function (not available).\nŒ® = SInDy(X‚Çô[:, 5:end], LÃÑ[:, 5:end], basis, Œª, opt = opt, maxiter = 10000,\rf_target = f_target)\r This time we succeded as we recovered the missing terms of each equation.\n$$du_1 = \\beta u_1u_2 $$ $$du_2 = \\gamma u_1u_2 $$\nAnd we also succeded using derivative data.\n# Test on uode derivative data\rprintln(\u0026quot;SINDY on learned, partial, available data\u0026quot;)\rŒ® = SInDy(X‚Çô[:, 2:end], LÃÇ[:, 2:end], basis, Œª, opt = opt, maxiter = 10000, normalize = true, denoise = true, f_target = f_target)\r Now we can extract the parameters.\npÃÇ = parameters(Œ®)\rprintln(\u0026quot;First parameter guess : $(pÃÇ)\u0026quot;)\runknown_sys = ODESystem(Œ®)\runknown_eq = ODEFunction(unknown_sys)\r The equations are recovered, but the parameters may not be the best, we can start another sindy run to get closer to the ground truth.\n# Just the equations\rb = Basis((u, p, t)-\u0026gt;unknown_eq(u, [1.; 1.], t), u)\r# Retune for better parameters -\u0026gt; we could also use DiffEqFlux or other parameter estimation tools here.\rŒ®f = SInDy(X‚Çô[:, 2:end], LÃÇ[:, 2:end], b, opt = STRRidge(0.01), maxiter = 100, convergence_error = 1e-18) # Succeed\rprintln(Œ®f)\rpÃÇ = parameters(Œ®f)\rprintln(\u0026quot;Second parameter guess : $(pÃÇ)\u0026quot;)\r# Create function\rrecovered_sys = ODESystem(Œ®f)\rrecovered_eq = ODEFunction(recovered_sys)\r# Build a ODE for the estimated system\rfunction dudt(du, u, p, t)\r# Add SInDy Term\rŒ±, Œ¥, Œ≤, Œ≥ = p\rz = recovered_eq(u, [Œ≤; Œ≥], t)\rdu[1] = Œ±*u[1] + z[1]\rdu[2] = -Œ¥*u[2] + z[2]\rend\r# Create the approximated problem and solution\rps = [p_[[1,4]]; pÃÇ]\rapproximate_prob = ODEProblem(dudt, u0, tspan, ps)\rapproximate_solution = solve(approximate_prob, Tsit5(), saveat = 0.01)\r# Look at long term prediction\rt_long = (0.0, 50.0)\rapproximate_prob = ODEProblem(dudt, u0, t_long, ps)\rapproximate_solution_long = solve(approximate_prob, Tsit5(), saveat = 0.1) # Using higher tolerances here results in exit of julia\rtrue_prob = ODEProblem(lotka, u0, t_long, p_)\rtrue_solution_long = solve(true_prob, Tsit5(), saveat = approximate_solution_long.t)\r Why are neural networks so good? because for large enough neural network, local optima are global optima.\n\rClick here to see more about Universal differential equations\nPhysics-Informed Neural Networks Solve and ODE with a neural network  Let $u\u0026rsquo; = f(u,t)$ with $u(0) = u_0$. We want to build a neural network $NN(t)$ that is the solution to this differential equation. By definition then, we must have that $NN\u0026rsquo;(t) = f(NN(t),t)$ and $NN(0) = u_0$. Define $\\mathcal{C}(\\theta) = \\displaystyle \\sum_{t} \\lVert NN\u0026rsquo;(t) - f(NN(t),t) \\rVert$ for $\\theta$ the parameters of the ODE.  Then this cost is zero when $NN(t)$ is the solution to the ODE Therefore we aim to minimize this loss to get the solution.   Extra trick: We can use $g(t) = tNN(t) + u_0$ as a test function instead of $NN(t)$. Notice that it is an approximator that always satisfies the boundary condition.  Example Let\u0026rsquo;s solve the ODE: $$u\u0026rsquo;(t) = cos(2\\pi t) = f(u,t)$$ with initial condition $$u(0) = 1$$\nFollowing the steps above, we create a neural network, a function $g(t)$ which we derivate and compute the $l_2$ norm of the difference between $g\u0026rsquo;(t)$ and $u\u0026rsquo;(t)$. In order to do this, we define $\\epsilon$ as small as the precision of a Float32 allow us.\nusing Flux\rNNODE = Chain( x -\u0026gt; [x],\rDense(1,32,tanh),\rDense(32,1),\rfirst)\rNNODE(1.0)\rg(t) = t*NNODE(t) + 1f0\rusing Statistics\rœµ = sqrt(eps(Float32))\rloss() = mean(abs2( ( ( g(t+œµ) - g(t) )/œµ ) - cos(2œÄ*t) ) for t in 0f0:0.01f0:1f0)\r We create a callback and train the neural network using a descent algorithm.\niter = 0\rcb = function()\rglobal iter += 1\rif iter % 500 == 0\rdisplay( loss() )\rend\rend\ropt = Flux.Descent(0.01) data = Iterators.repeated( (), 5000 )\rFlux.train!(loss, Flux.params(NNODE), data, opt; cb= cb)\r Now we can compare to the real solution:\n$$ u(t) = \\int u\u0026rsquo;(t) dt = \\int cos(2\\pi t)dt = \\dfrac{sin(2\\pi t)}{2\\pi} + constant$$\nbut $u(0) = 1$ therefore we get $u(t) = 1 + \\dfrac{sin(2\\pi t)}{2\\pi}$\n\r\rTrue Solution vs Neural Network\r\r\rWhy Physics- Informed Neural Networks?\n $\\mathcal{C}(\\theta) = C_{pde}(\\theta) + C_{boundary} + C_{data}(\\theta)$ can nudge a model towards data.  Equivalent to regularizing the neural network by a scientific equation   Can train fast continuous surrogates by making the neural netowrk parameter dependent.  Example: Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation For this problem we will use NeuralPDE.jl (\rCheck more here). Following this steps:\n Write the function and equation. Make $\\sigma^\\top \\nabla u (t,X)$ a neural network. Solve the resulting SDEs and learn $\\sigma^\\top \\nabla u$ via:   using NeuralPDE\rusing Flux\rusing DifferentialEquations\rusing LinearAlgebra\rd = 100 # number of dimensions\rX0 = fill(0.0f0, d) # initial value of stochastic control process\rtspan = (0.0f0, 1.0f0)\rŒª = 1.0f0\rg(X) = log(0.5f0 + 0.5f0 * sum(X.^2))\rf(X,u,œÉ·µÄ‚àáu,p,t) = -Œª * sum(œÉ·µÄ‚àáu.^2)\rŒº_f(X,p,t) = zero(X) # Vector d x 1 Œª\rœÉ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d\rprob = TerminalPDEProblem(g, f, Œº_f, œÉ_f, X0, tspan)\rhls = 10 + d # hidden layer size\ropt = Flux.ADAM(0.01) # optimizer\r# sub-neural network approximating solutions at the desired point\ru0 = Flux.Chain(Dense(d, hls, relu),\rDense(hls, hls, relu),\rDense(hls, 1))\r# sub-neural network approximating the spatial gradients at time point\rœÉ·µÄ‚àáu = Flux.Chain(Dense(d + 1, hls, relu),\rDense(hls, hls, relu),\rDense(hls, hls, relu),\rDense(hls, d))\rpdealg = NNPDENS(u0, œÉ·µÄ‚àáu, opt=opt)\r@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,\ralg=EM(), dt=1.2, pabstol=1f-2)\r We can solve this high dimensional problem in only $22.623969$ seconds $523.95 M$ allocations: $36.683 GiB$, $17.95%$ gc time.\n","date":1596178587,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596178587,"objectID":"d9dae3dbd2e0d9facdc4570f95a7858c","permalink":"https://danpereda.github.io/post/scientificmachinelearning/","publishdate":"2020-07-31T02:56:27-04:00","relpermalink":"/post/scientificmachinelearning/","section":"post","summary":"Here is what I\u0026rsquo;ve learned from the WorkShop Doing Scientific Machine Learning (SciML) With Julia from Chris Rackauckas. There is also an MIT course and Workshop exercises (with solutions) by the same author about this subject that I\u0026rsquo;ve been checking out and strongly recomend.","tags":["JuliaCon 2020"],"title":"Scientific Machine Learning on Julia","type":"post"},{"authors":[],"categories":[],"content":"This is what I\u0026rsquo;ve learned from the workshop by David P. Sanders\nWe will simulate the dynamics of an epidemic, i.e, an outbreak of an infectious disease. In a population of people with N individuals we will be interested in how the number of susceptible (S), infectious (I) and recovered (R) individual changes over time. We will begin by looking at simple models that take into account only total numbers of people, by the end of the workshop we should be able to structure a more complicated individual - based or agent - based simulation, where we model individual people moving around space and interacting with one another.\nFor Simplicity, those individuals will be modelled as random walks on a grid, i.e, points that choose a neighbouring grid point at random to jump to.\nTable of Contents  Generic programming Composite types (Outer) constructors Generic programming with Types Types for agents Composition and Parametrised Types Spatial SIR model Dynamics  Generic programming: Random walks Each step roughly corresponds to a different function. Each different type of walker will need a different way to:\n initialize() itself and then move() which will return the new position chosen by the walker.  Therefore a walk of length T is given by the following function\nfunction walk(T)\rpos = initialize()\rtrajectory = [pos] # make a Vector that contains just the current value of `pos`\rfor t in 1:T\rnew_pos = move(pos)\rpush!(trajectory, new_pos) # append to the Vector\rpos = new_pos # update for next iteration\rend\rreturn trajectory\rend\r We noticed that this depends on the functions initialize() and move() that should be defined on the global scope. Since a random walk can be in n dimension, we would like to be able to run the same function of all dimension, this is what is called generic programming.\nfunction walk(initialize, move, T)\rpos = initialize()\rtrajectory = [pos]\rfor t in 1:T\rpos = move(pos) # *update* the value pointed to by `pos`\rpush!(trajectory, deepcopy(pos)) # at the cost of needing to copy `pos` when we store it if it is a vector\rend\rreturn trajectory\rend\r This way we can have different initialize() and move() functions depending on the dimension of the walker and we will be able to recover the trajectory calling the same walk function.\nNow the question is, how can we efficiently store information about each walker? we would like to now not only the trajectory but also if he is susceptible, infected or recovered. This leads us to the following section.\nComposite types The main idea is to collect up or aggregate all relevant information into a new data structure, called a composite type (or custom type, aggregate type, user-defined type, \u0026hellip;).\nBasically we want to be able to specify the \u0026ldquo;template\u0026rdquo; / \u0026ldquo;shape\u0026rdquo; / \u0026ldquo;structure\u0026rdquo; for a bag or box that will contain all the relevant information; this specification is the type itself. Then we need to produce objects which have that structure, i.e. which contain the corresponding variables; these are called instances.\nIn Julia this is accomplished using the struct keyword (short for \u0026ldquo;structure\u0026rdquo;). For example, we can make an object that contains the $x$ and $y$ coordinates of a walker in 2 dimensions as:\nstruct Walker2D\rx::Int64\ry::Int64\rend\r (Outer) constructors Suppose we want walkers to be born at the origin unless otherwise stated. We don\u0026rsquo;t want to have to write Walker2D(0, 0) each time; we would like to just write Walker2D(). In other words, we want to add a new method to the function Walker2D:\nWalker2D() = Walker2D(0, 0)\nSuch a constructor is called an outer constructor, since it lives outside the definition of the type.\nMaking walkers move We are not allowed to modify the fields of a walker because we defined the structure as being immutable (if we want it to be mutable we need to specify it). Usually this will give us better performance. So in order to make our walker move, we need to create a new object with the new position. This could seem expensive, but in fact the Julia compiler will often be able to completely remove this object creation and produce code that is just as efficient as if there were no object at all!\nSuppose we want to only move on the $ x - axis $ then we can just define:\nmove(w::Walker2D) = Walker2D(w.x + rand( (-1,1) ), w.y)\r Now supposed we need to defined a function that moves us to an adjacent point at random, then we can just throw a coin an choose a direction based on that result.\nfunction jump(w::Walker2D)\rr = rand()\rif r \u0026gt; 0.5\rreturn Walker2D(w.x + rand( (-1,1) ), w.y)\relse\rreturn Walker2D(w.x, w.y + rand( (-1,1) ) )\rend\rend\r Generic programming with Types Before we create a walk function that depends on the functions initialize() and move(), but what if we just have one of each function with different methods? this solution should be better, otherwise we would have to define functions initialize_1D() and initialize_2D() to pass it as an argument an make a distinction between 1 dimension and 2 dimension walkers.\n\u0026quot;Calculate the trajectory of a walker `w` for time `T`.\u0026quot;\rfunction walk(w, T)\rtrajectory = [w] # store the current (initial) position of `w`\rfor t in 1:T\rw = move(w) # update the value bound to `w`\rpush!(trajectory, deepcopy(w)) # store the current value\rend\rreturn trajectory\rend\r We have not specified a type of $w$ this means that if we have a move function that works for instance for Integer numbers (BigInt, Int64 and so on) it should work and it should also work if we have a Walker2D as an argument.\nTypes for agents We are getting towards our goal of putting everything together to make a model of people moving around and interacting with one another. Most people start off susceptible, but when an infectious person meets a susceptible the infection is transmitted with a certain probability.\nWe will make an individual-based model, also called an agent-based model. We need a struct called Agent that contains whatever information an agent needs. In our case we will need a position and an infection status.\nThe position will behave almost like a normal random walk that we have seen before, while the infection status needs to reflect whether the agent is susceptible (S), infectious (I) or recovered / removed (R).\nEnums We could represent the infection status simply using an integer, e.g. 0, 1 or 2. But then our code will be hard to read, since we will be comparing the infection status to numbers all the time without remembering which one is which.\nA nice solution is just to use @enums macro.\n@enum InfectionStatus S=1 I R # specify that `S` corresponds to the value 1\r We will have a new Type InfectionStatus, with possible values S, I and R that also store a numerical value $ S = 1 $, $ I = 2$ and $ R = 3$. Then we can do Int(I) and it will return the integer 2, we can also do\nstatus = I\rif status == I\rprintln(\u0026quot;infected!\u0026quot;)\rend\r and get infected! as a result. This way the InfectionStatus information gets easy to manipulate and remember in our code.\nComposition and Parametrised Types We can place one object inside another one.\nSuppose we have defined a SimpleWalker2D structure as follows.\nstruct SimpleWalker2D\rx::Int64\ry::Int64\rend\r Then we can define an Agent as:\nstruct Agent\rposition::SimpleWalker2D\rstatus::InfectionStatus\rend\r Then we can create an infected Agent in position $(1,2)$ by simply doing  w = SimpleWalker2D(1,2) and then a = Agent(w,I).\nAs we learned before, we would like to have our program a bit more generic. One way of doing it is by parametrizing Types:\nstruct Agent{T}\rposition::T\rstatus::InfectionStatus\rend\r Sometimes beeing too generic can cause troubles if not careful. Then we can parametrise for only some Types. Suppose there is a common abstract type AbstractWalker for all of the possible types that we want to be able to use for T (this can be 1,2 and 3 dimension walkers for example), then we can write:\nstruct Agent{T \u0026lt;: AbstractWalker}\rposition::T\rstatus::InfectionStatus\rend\r Spatial SIR model Now we are ready to build the spatial model. It will consist of walkers moving in a 2D box. This was an exercise left to the audience at the end of the talk, so we will solve it as it is written on the notebook.\nConfinement inside a box We need agents to live inside a box so that they don\u0026rsquo;t disperse.\nExercise  Make a ConfinedWalker2D type. Its fields are a Walker2D object and a box size, L.  struct Walker2D\rx::Int64\ry::Int64\rend\r struct ConfinedWalker2D\rw::Walker2D\rL::Int64\rend\r The important part here is that we just give the size of the box as a parameter. We do not do an inner constructor that checks if the position of the walker is inside the box. This is because inner constructors can be bothersome so we just need to keep in mind that we should check boundaries at some future function.\n Extend move to ConfinedWalker2D. If the walker tries to jump outside the box, i.e. outside the sites 1 to ùêø , in either direction, then it remains where it is.  function move(cw::ConfinedWalker2D)\rr = rand()\rstep= rand([-1,1])\rif r \u0026gt; 0.5\rposx = cw.w.x + step\rposy = cw.w.y\relse\rposx = cw.w.x\rposy = cw.w.y + step\rend\rif (posx \u0026lt;= cw.L)\u0026amp;\u0026amp; (1 \u0026lt;= posx)\u0026amp;\u0026amp;(posy \u0026lt;= cw.L)\u0026amp;\u0026amp; (1 \u0026lt;= posy)\rw = Walker2D(posx,posy)\rreturn ConfinedWalker2D(w, cw.L)\relse\rreturn cw\rend\rend\r  Make a confined Agent and calculate and draw its trajectory to make sure it stays inside the box.  struct Agent{T}\rcw::T\rstatus::InfectionStatus\rend\r Let\u0026rsquo;s consider $L = 6$ and initial position $(5,5)$\n\r\rTrajectory\r\r\rWe can see how it does not move outside the border and stays in the same position in the $16th$ move for example.\nInitialization Exercises  Write a function initialize that takes parameters ùêø, the box length, and ùëÅ, the number of agents. It builds, one by one, a Vector of agents, by proposing a position for each one and checking if that position is already occupied. If it is occupied, it should generate another one, and so on until it finds a free spot. All of the agents should have state S, except for one infectious individual (I).  To do this you should write a function check_occupied that checks if a particular position is occupied.\nfunction check_ocupied(w::Walker2D,v)\rm = length(v)\rif m == 0\rreturn false\relse\rfor i = 1:m\rif (w.x == v[i].cw.w.x) \u0026amp;\u0026amp; (w.y == v[i].cw.w.x)\rreturn true\rend\rend\rreturn false\rend\rend\r function initialize(L,N)\ri= 0\rv = []\rwhile i \u0026lt; N\rx = rand(-L:L)\ry = rand(-L:L)\rw = Walker2D(x,y)\rif !check_ocupied(w,v)\ra = Agent( ConfinedWalker2D(w,L), S)\rpush!(v,deepcopy(a))\ri = i+1\rend\rend\rindex = rand(1:N)\rv[index] = Agent(v[index].position,I)\rreturn v\rend\r  Write a function visualize_agents that takes in a collection of agents as argument. It should plot a point for each agent, coloured according to its status  function visualize_agents(v)\rm = length(v)\rx = SA[zeros(m)]\ry = SA[zeros(m)]\rinfection_status = []\rfor i = 1:m\rx[1][i] = v[i].cw.w.x\ry[1][i] = v[i].cw.w.y\rpush!(infection_status,deepcopy(Int(v[i].status)))\rend\rreturn scatter((x,y) , c = infection_status, ratio =1, leg = false)\rend\r  Run these functions to visualize the initial condition.  Let\u0026rsquo;s consider $L = 6$ and $N = 20$. Then we get the following:\n\r\rInitial condition\r\r\rDynamics Now we just need to simulate the dynamics of the system. We will consider parameters $p_l$ and $p_R$, the probabilities of infection and recovery at each time step, respectively. The system evolves as follows:\n First the system is initialized with only one random infected agent. Secondly, a single agent choosen randomly, call it $i$ tries to move to an adjancent position. If the position is ocuppied, by agent $j$, then neither of them move, but they interact as follows: If agent $i$ is infected and agent $j$ is susceptible then agent $j$ gets infected with probability $p_I$ If agent $i$ is infected, it recovers with probability $p_R$.  We do this part defining step() function\nNotice that this model does not allow a recovered agent to get infected again.\nThen the step! function looks as follows:\n\u0026quot;Receives a vector of agents and the required probabilities,\rreturns a modified agent vector\u0026quot;\rfunction step!(v, pI, pR)\rn = length(v)\ri = rand(1:n)\rcwi = move(v[i].cw) # we move agent i index = check_ocupied2(cwi.w,v) # Give us [] or the index j of agent\rm = length(index) # can be 0 or 1\rif m == 0\rv[i] = Agent(cwi, v[i].status)\relse\rfor j in index\rv[i],v[j] = infection(v[i],v[j], pI, pR)\rend\rend\rreturn v\rend\r Infection() function makes the interaction between the agents $i$ and $j$ following the previous rules.\nLet\u0026rsquo;s see how the system evolves after $1000$ steps. For this we will use $L = 6$ , $N = 30$, $p_I = 0.5$ and $p_R = 0.05$. Orange means infected, Blue is susceptible and Green is recovered.\n\r\rSystem Evolution\r\r\r\r\rSIR\r\r\rFinal thoughts This workshop and to be honest most of JuliaCon 2020 workshops were amazing, as you can learn so much. There is a known saying about Mathematicians being able to code and solve problems at the cost of doing a really long, slow and bad algorithm. Going to this workshops and trying to learn as much Julia as possible during JuliaCon is making me improve a lot in terms of coding and knowing the capabilities of the language, so I recomend checking the workshops videos on youtube\n","date":1595973432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596059832,"objectID":"2428ec7a948d62d1e37f65fbab8a7988","permalink":"https://danpereda.github.io/post/test/","publishdate":"2020-07-28T17:57:12-04:00","relpermalink":"/post/test/","section":"post","summary":"An overview of what I've learned from the workshop","tags":["JuliaCon 2020","COVID - 19"],"title":"Learn Julia via epidemic modelling","type":"post"},{"authors":[],"categories":null,"content":"","date":1575376251,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575376251,"objectID":"232776fc7bdd03b04c163d063335c065","permalink":"https://danpereda.github.io/talk/pgmo/","publishdate":"2019-12-03T20:32:51-04:00","relpermalink":"/talk/pgmo/","section":"talk","summary":"","tags":[],"title":"Bilevel optimization applied to strategic pricing in electricity markets and extension to markets with massive entry of renewable energies","type":"talk"}]