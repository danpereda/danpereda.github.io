<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Danpereda</title>
    <link>https://danpereda.github.io/post/</link>
      <atom:link href="https://danpereda.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 18 Jun 2021 18:08:02 -0400</lastBuildDate>
    <image>
      <url>https://danpereda.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://danpereda.github.io/post/</link>
    </image>
    
    <item>
      <title>Hand tracking and volume gesture controller</title>
      <link>https://danpereda.github.io/post/handtracking/</link>
      <pubDate>Fri, 18 Jun 2021 18:08:02 -0400</pubDate>
      <guid>https://danpereda.github.io/post/handtracking/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We will first lean how to use our webcam video as an input for a Hand tracking model, and we will modularize
it in order to make it easy to use in other projects. As a test, we will build a volume gesture controller, such that we
can control the volume of our computer using hand gestures. The result will be will like this (but in real time).&lt;/p&gt;





  











&lt;figure id=&#34;figure-volume-gesture-controller--hand-tracking&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/HandTracking/Volumne.png&#34; data-caption=&#34;Volume Gesture Controller &amp;#43; Hand tracking&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/HandTracking/Volumne.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Volume Gesture Controller + Hand tracking
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The learning material as well as the project idea can be found mainly in 
&lt;a href=&#34;https://www.computervision.zone/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computervision.zone&lt;/a&gt;, 
&lt;a href=&#34;https://google.github.io/mediapipe/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mediapipe&lt;/a&gt; and

&lt;a href=&#34;https://docs.opencv.org/4.5.2/d6/d00/tutorial_py_root.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;opencv&lt;/a&gt;. The main difference is that I will be exploring and explaining
step by step from the deep learning model to the coding.&lt;/p&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Setting up environment.&lt;/li&gt;
&lt;li&gt;Basics: Read the webcam.&lt;/li&gt;
&lt;li&gt;Hand Detection and Tracking using Mediapipe
&lt;ul&gt;
&lt;li&gt;Mediapipe Hands Model.&lt;/li&gt;
&lt;li&gt;Palm Detection Model.&lt;/li&gt;
&lt;li&gt;Hand Landmark Model.&lt;/li&gt;
&lt;li&gt;Coding it.&lt;/li&gt;
&lt;li&gt;Modularize.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Volume Gesture Controller using Hand Tracking module&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;setting-up-environment&#34;&gt;Setting up environment&lt;/h1&gt;
&lt;p&gt;Install the requirements&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;pip install opencv-contrib-python
pip install mediapipe
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;basics-read-the-camera&#34;&gt;Basics: Read the camera&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2 # opencv
import sys  # Python Standard Library System-specific parameters and functions

# if we have more than one camera, we select one with the variable s
s = 0
if len(sys.argv) &amp;gt; 1:
    s = sys.argv[1]

# Define camera we are going to read
source = cv2.VideoCapture(s)
# Create a window for the camera
win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

# While we don&#39;t press the &amp;quot;ESC&amp;quot; key the window will remain open
# and we will show the frame in the window previously defined
while cv2.waitKey(1) != 27: 
    success, frame = source.read()
    # if we don&#39;t read the image successfully break the loop
    if not success:
        break
    # Otherwise show the frame on the window
    cv2.imshow(win_name, frame)
    
# Since we exit the loop, its time to clean resources
source.release()
cv2.destroyWindow(win_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;hand-detection-and-tracking-using-mediapipe&#34;&gt;Hand Detection and Tracking using Mediapipe&lt;/h1&gt;
&lt;p&gt;I will give a brief description on the Mediapipe models and focus on how to use them together with opencv.
Also, I will be using images and videos from the documentation at 
&lt;a href=&#34;https://google.github.io/mediapipe/solutions/hands#python-solution-api&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Mediapipe&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;mediapipe-hands-model&#34;&gt;Mediapipe Hands Model&lt;/h2&gt;
&lt;p&gt;MediaPipe Hands is a high-fidelity hand and finger tracking solution.
It employs machine learning (ML) to infer 21 3D landmarks of a hand from just a single frame. Its ML Pipeline consists of multiple models working together.
A palm detection model that operates on the full image and returns an oriented hand bounding box.
A hand landmark model that operates on the cropped image region defined by the palm detector and returns high-fidelity 3D hand keypoints.
In addition, in the pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand.&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
&lt;img src = &#34;https://google.github.io/mediapipe/images/mobile/hand_tracking_3d_android_gpu.gif&#34;&gt;
&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
Fig.1 - Tracked 3D hand landmarks are represented by dots in different shades, with the brighter ones denoting landmarks closer to the camera.
&lt;/p&gt;
&lt;h3 id=&#34;palm-detection-model&#34;&gt;Palm Detection Model&lt;/h3&gt;
&lt;p&gt;The first step will be detecting a palm, since estimating bounding boxes of rigid objects like palms and fists is significantly simpler than detecting hands
with articulated fingers. For this task, a single-shot detector has been used (
&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SSD: Single Shot MultiBox Detector Paper&lt;/a&gt;).
In addition, as palms are smaller objects and the single-shot detector will create many boxes,
some technique to select the correct entity must be used,
for this problem the non-maximum supression algorithm works well even for two-hand-self-occlusion cases, like handshakes.
Moreover, palms can be modelled using square bounding boxes (anchors in ML terminology) ignoring other aspect ratios,
and therefore reducing the number of anchors by a factor of 3-5. Second, an encoder-deocoder feature extractor is used for
bigger scene context awareness even for small objects.&lt;/p&gt;
&lt;p&gt;The average precision of this palm detection model is 95.7%.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note on The Non Maximum Supression (NMS) algorithm&lt;/strong&gt; : is a technique used in many computer vision algorithms.
It is a class of algorithms to select one entity
(e.g. bounding boxes) out of many overlapping entities.
The selection criteria can be chosen to arrive at particular results.
Most commonly, the criteria is some form of probability number along with some form of overlap measure (e.g. IOU).

&lt;a href=&#34;https://whatdhack.medium.com/reflections-on-non-maximum-suppression-nms-d2fce148ef0a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read more about it here&lt;/a&gt;&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
&lt;img src = &#34;https://miro.medium.com/max/838/1*8EoRC_Xu625eVAquP9ga5w.png&#34;&gt;
&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
NMS
&lt;/p&gt;
&lt;p&gt;Finally we can see the computation graph of the model.&lt;/p&gt;





  











&lt;figure id=&#34;figure-palm-detection-graph&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/HandTracking/PalmTensor.png&#34; data-caption=&#34;Palm detection graph&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/HandTracking/PalmTensor.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Palm detection graph
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;hand-landmark-model&#34;&gt;Hand Landmark Model&lt;/h3&gt;
&lt;p&gt;After the palm detection over the whole image, the hand landmark model performs precise keypoint
localization of 21 3D hand-knuckle coordinates inside the detected hand regions via regression, that is direct
coordinate prediction. The model learns a consistent internal hand pose representation and is robust even to
partially visible hands and self-occlusions.&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
&lt;img src = &#34;https://google.github.io/mediapipe/images/mobile/hand_landmarks.png&#34;&gt;
&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
Fig.2 - Hand Landmarks
&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
&lt;img src = &#34;https://google.github.io/mediapipe/images/mobile/hand_crops.png&#34;&gt;
&lt;/p&gt;
&lt;p align = &#34;center&#34;&gt;
Fig.3 - Top: Aligned hand crops passed to the tracking network with ground truth annotation. Bottom: Rendered synthetic hand images with ground truth annotation.
&lt;/p&gt;
&lt;p&gt;We can see the computation graph here.&lt;/p&gt;





  











&lt;figure id=&#34;figure-hand-tracking-graph&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/HandTracking/HandGraph.png&#34; data-caption=&#34;Hand Tracking Graph&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/HandTracking/HandGraph.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Hand Tracking Graph
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;coding-it&#34;&gt;Coding it&lt;/h2&gt;
&lt;p&gt;First lets take a look at the &lt;strong&gt;Hands&lt;/strong&gt; class provided by Mediapipe to check what we need.
For instance, as opencv reads images in &lt;code&gt;BGR&lt;/code&gt; format, we need to check if this class needs an &lt;code&gt;RGB&lt;/code&gt; one.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Hands(SolutionBase):
  &amp;quot;&amp;quot;&amp;quot;MediaPipe Hands.

  MediaPipe Hands processes an RGB image and returns the hand landmarks and
  handedness (left v.s. right hand) of each detected hand.

  Note that it determines handedness assuming the input image is mirrored,
  i.e., taken with a front-facing/selfie camera (
  https://en.wikipedia.org/wiki/Front-facing_camera) with images flipped
  horizontally. If that is not the case, use, for instance, cv2.flip(image, 1)
  to flip the image first for a correct handedness output.

  Please refer to https://solutions.mediapipe.dev/hands#python-solution-api for
  usage examples.
  &amp;quot;&amp;quot;&amp;quot;

  def __init__(self,
               static_image_mode=False,
               max_num_hands=2,
               min_detection_confidence=0.5,
               min_tracking_confidence=0.5):
    &amp;quot;&amp;quot;&amp;quot;Initializes a MediaPipe Hand object.

    Args:
      static_image_mode: Whether to treat the input images as a batch of static
        and possibly unrelated images, or a video stream. See details in
        https://solutions.mediapipe.dev/hands#static_image_mode.
      max_num_hands: Maximum number of hands to detect. See details in
        https://solutions.mediapipe.dev/hands#max_num_hands.
      min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for hand
        detection to be considered successful. See details in
        https://solutions.mediapipe.dev/hands#min_detection_confidence.
      min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the
        hand landmarks to be considered tracked successfully. See details in
        https://solutions.mediapipe.dev/hands#min_tracking_confidence.
    &amp;quot;&amp;quot;&amp;quot;

  def process(self, image: np.ndarray) -&amp;gt; NamedTuple:
    &amp;quot;&amp;quot;&amp;quot;Processes an RGB image and returns the hand landmarks and handedness of each detected hand.

    Args:
      image: An RGB image represented as a numpy ndarray.

    Raises:
      RuntimeError: If the underlying graph throws any error.
      ValueError: If the input image is not three channel RGB.

    Returns:
      A NamedTuple object with two fields: a &amp;quot;multi_hand_landmarks&amp;quot; field that
      contains the hand landmarks on each detected hand and a &amp;quot;multi_handedness&amp;quot;
      field that contains the handedness (left v.s. right hand) of the detected
      hand.
    &amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here we see a few things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We will have to convert the image using &lt;code&gt;cv2.cvtColor(img,cv2.COLOR_BGR2RGB)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;To define a Hands object we need to access to &lt;code&gt;mediapipe.solutions.hands.Hands(*Args)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;We need to call the &lt;code&gt;process&lt;/code&gt; function to run the inference model in our image,
and get the results by calling &lt;code&gt;.multi_hand_landmarks&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this we will have our results as coordinates, as we saw in the Hand Landmark Model section
there are 21 landmark points so there must be some function to automatically draw the landmarks and the conections
between them right? Indeed, if we check &lt;code&gt;mediapipe.solutions.drawing_utils&lt;/code&gt; we can see the following
method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def draw_landmarks(
    image: np.ndarray,
    landmark_list: landmark_pb2.NormalizedLandmarkList,
    connections: Optional[List[Tuple[int, int]]] = None,
    landmark_drawing_spec: DrawingSpec = DrawingSpec(color=RED_COLOR),
    connection_drawing_spec: DrawingSpec = DrawingSpec()):
  &amp;quot;&amp;quot;&amp;quot;Draws the landmarks and the connections on the image.

  Args:
    image: A three channel RGB image represented as numpy ndarray.
    landmark_list: A normalized landmark list proto message to be annotated on
      the image.
    connections: A list of landmark index tuples that specifies how landmarks to
      be connected in the drawing.
    landmark_drawing_spec: A DrawingSpec object that specifies the landmarks&#39;
      drawing settings such as color, line thickness, and circle radius.
    connection_drawing_spec: A DrawingSpec object that specifies the
      connections&#39; drawing settings such as color and line thickness.

  Raises:
    ValueError: If one of the followings:
      a) If the input image is not three channel RGB.
      b) If any connetions contain invalid landmark index.
  &amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to run code it, lets get an image from our webcam, run the inferece model
on it an finally draw the landmarks and conections.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import sys
import mediapipe as mp

s = 0
if len(sys.argv) &amp;gt; 1:
    s = sys.argv[1]

source = cv2.VideoCapture(s)
win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

mp_hands = mp.solutions.hands
hands = mp_hands.Hands() 
mp_draw = mp.solutions.drawing_utils

while cv2.waitKey(1) != 27:
    success, frame = source.read()
    if not success:
        break
    imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    imgRGB.flags.writeable = False
    # Run inference model on the RGB image
    results = hands.process(imgRGB)
    # We will like to know the hand_landmarks of all hands detected
    if results.multi_hand_landmarks:
        for hand_landmark in results.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, hand_landmark, mp_hands.HAND_CONNECTIONS)
    
    cv2.imshow(win_name, frame)
    
source.release()
cv2.destroyWindow(win_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets say we want to access some specific landmarks, can we do it? the answer is yes,
we can get the &lt;code&gt;ID&lt;/code&gt; and relative coordinates &lt;code&gt;(x,y)&lt;/code&gt; accesing to the &lt;code&gt;.landmark&lt;/code&gt;
method.&lt;/p&gt;
&lt;p&gt;In the following code I draw some special circles on the landmarks 4 and 20 (see picture in Hand Landmark Model)
and added some Frames per second count on the top left corner.&lt;/p&gt;





  











&lt;figure id=&#34;figure-hand-tracking-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/HandTracking/Hand.png&#34; data-caption=&#34;Hand Tracking Result&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/HandTracking/Hand.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Hand Tracking Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import sys
import time
import mediapipe as mp

s = 1
if len(sys.argv) &amp;gt; 1:
    s = sys.argv[1]

source = cv2.VideoCapture(s)
win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

mp_hands = mp.solutions.hands
hands = mp_hands.Hands() 
mp_draw = mp.solutions.drawing_utils

# To make FPS count
previous_time = time.time()

while cv2.waitKey(1) != 27:
    success, frame = source.read()
    if not success:
        break
    imgRGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # To improve performance, optionally mark the image as not writeable to
    # pass by reference.
    imgRGB.flags.writeable = False
    # Run inference model on the RGB image
    results = hands.process(imgRGB)
    # We will like to know the hand_landmarks of all hands detected
    if results.multi_hand_landmarks:
        for hand_landmark in results.multi_hand_landmarks:
            # If we want to do things with an specifict landmark
            for id_landmark, landmark in enumerate(hand_landmark.landmark):
                # The coordinates of the landmarks are relative to the height and width
                height, width, channels = frame.shape
                center_x, center_y = int(landmark.x * width), int(landmark.y * height)
                if id_landmark == 4:
                    cv2.circle(frame, (center_x, center_y), 15, (255, 0, 255), cv2.FILLED)
                if id_landmark == 20:
                    cv2.circle(frame, (center_x, center_y), 15, (255, 255, 0), cv2.FILLED)

            mp_draw.draw_landmarks(frame, hand_landmark, mp_hands.HAND_CONNECTIONS)

    # Display FPS count
    current_time = time.time()
    fps = int(1/(current_time - previous_time))
    previous_time = current_time
    cv2.putText(frame, str(fps), (10,70), cv2.FONT_ITALIC, 3, (255, 0, 255), thickness = 2)

    cv2.imshow(win_name, frame)
    
source.release()
cv2.destroyWindow(win_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;modularize-all-we-have-learned-today&#34;&gt;Modularize all we have learned today&lt;/h2&gt;
&lt;p&gt;As we saw before, we are recycling the same code time over time, for this reason,
it will come in handy to modularize what we have done so that we can use it on other
projects as well.&lt;/p&gt;
&lt;p&gt;The module will look like this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import time
import mediapipe as mp

class handDetector():
    def __init__(self,
                 static_image_mode=False,
                 max_num_hands=2,
                 min_detection_confidence=0.5,
                 min_tracking_confidence=0.5):

        self.static_image_mode = static_image_mode
        self.max_num_hands = max_num_hands
        self.min_detection_confidence = min_detection_confidence
        self.min_tracking_confidence = min_tracking_confidence
        # Initializing Hand model and drawing utils
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(self.static_image_mode,
                                         self.max_num_hands,
                                         self.min_detection_confidence,
                                         self.min_tracking_confidence)
        self.mp_draw = mp.solutions.drawing_utils

    def findHands(self, img):
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        imgRGB.flags.writeable = False
        # Run inference model on the RGB image
        self.results = self.hands.process(imgRGB)
        if self.results.multi_hand_landmarks:
            for hand_landmark in self.results.multi_hand_landmarks:
                 self.mp_draw.draw_landmarks(img, hand_landmark, self.mp_hands.HAND_CONNECTIONS)
        return img

    def findPosition(self, img, hand_number = 0):
        landmark_list = []
        if self.results.multi_hand_landmarks:
            hand = self.results.multi_hand_landmarks[hand_number]
            for id_landmark, landmark in enumerate(hand.landmark):
                height, width, channels = img.shape
                center_x, center_y = int(landmark.x * width), int(landmark.y * height)
                landmark_list.append([id_landmark, center_x, center_y])
        return landmark_list

# This is for testing in the same module that everything works
def main():
    source = cv2.VideoCapture(1)
    win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
    cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)
    detector = handDetector()
    previous_time = time.time()
    while cv2.waitKey(1) != 27:
        success, frame = source.read()
        if not success:
            break
        img = detector.findHands(frame)
        landmark_list = detector.findPosition(img)
        if len(landmark_list) != 0:
            print(landmark_list[4])
        # Display FPS count
        current_time = time.time()
        fps = int(1 / (current_time - previous_time))
        previous_time = current_time
        cv2.putText(frame, str(fps), (10, 70), cv2.FONT_ITALIC, 3, (255, 0, 255), thickness=2)

        cv2.imshow(win_name, img)

    source.release()
    cv2.destroyWindow(win_name)

if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if we want to use it we just do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import mediapipe as mp
import HandTrackingModule

source = cv2.VideoCapture(1)
win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

detector = HandTrackingModule.handDetector()

while cv2.waitKey(1) != 27:
    success, frame = source.read()
    if not success:
        break
    img = detector.findHands(frame)

    cv2.imshow(win_name, img)

source.release()
cv2.destroyWindow(win_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;volume-gesture-controller&#34;&gt;Volume Gesture Controller&lt;/h1&gt;
&lt;p&gt;We have a hand tracking module already done, so let&amp;rsquo;s say we want to control the volume of our computer
by moving the thumb and index finger closer and further away from each other. From before we now the thumb is
landmark number 4 and the index is landmark number 8.&lt;/p&gt;
&lt;p&gt;Now we can think on creating a straight line between the landmark 4 and 8, and computing its length which will be proportional
to the volume. We need to be careful with the following, the length of this line might not be 0 even when we have our fingers touching
each other, because the landmark points are not on the edge and we don&amp;rsquo;t know what is the distance in pixels when they are the farthest
away from each other, therefore we will have to print the length and create a &lt;code&gt;UPPER_BOUND&lt;/code&gt;
and &lt;code&gt;LOWER_BOUND&lt;/code&gt; based on this, second, the volume range that our package will can be 0 to 100, but it can also be something else, in any case
we need to map our [&lt;code&gt;LOWER_BOUND&lt;/code&gt;, &lt;code&gt;UPPER_BOUND&lt;/code&gt;]  interval into [&lt;code&gt;MIN_VOL&lt;/code&gt;, &lt;code&gt;MAX_VOL&lt;/code&gt;].&lt;/p&gt;
&lt;p&gt;Finally, just to make it look prettier, we will add draw a circle in the middle point that will change color when both fingers are super close
to each other and a volume bar to the left of the screen.&lt;/p&gt;
&lt;h2 id=&#34;volume-controller-package&#34;&gt;Volume Controller Package&lt;/h2&gt;
&lt;p&gt;We will use 
&lt;a href=&#34;https://github.com/AndreMiras/pycaw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pycaw&lt;/a&gt; as it is pretty straightforward to use from the github repository
we can see the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(
    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))
volume.GetMute()
volume.GetMasterVolumeLevel()
volume.GetVolumeRange()
volume.SetMasterVolumeLevel(-20.0, None)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here we can see that the first lines are initialization of the devices and creating a &lt;code&gt;volume&lt;/code&gt; object, this one
has some methods, from which we are interested in the last two, &lt;code&gt;GetVolumeRange()&lt;/code&gt; and &lt;code&gt;SetMasterVolumeLevel(-20.0, None)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;finishing-it&#34;&gt;Finishing it!&lt;/h2&gt;
&lt;p&gt;Our final code will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2
import sys
import numpy as np
import time
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
import HandTrackingModule as htm

s = 0
if len(sys.argv) &amp;gt; 1:
    s = sys.argv[1]

# Define width and height of the webcam
width_cam, height_cam = 1280, 720

# Define camera we are going to read
source = cv2.VideoCapture(s)
source.set(3, width_cam) # Property number 3 : width
source.set(4, height_cam) # Property number 4: height
# Create a window for the camera
win_name = &amp;quot;Camera Preview: ESC to exit&amp;quot;
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

previous_time = 0
# Initialize detector
detector = htm.handDetector(max_num_hands=1,
                            min_tracking_confidence=0.7,
                            min_detection_confidence=0.7)

# Initialize Audio devices
devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = cast(interface, POINTER(IAudioEndpointVolume))

# Get Volume Range
volume_range = volume.GetVolumeRange()
MIN_VOLUME, MAX_VOLUME = volume_range[0], volume_range[1]

# Define constants
UPPER_BOUND = 300
LOWER_BOUND = 30

vol_bar = 400
vol_per = 0

while cv2.waitKey(1) != 27:
    success, frame = source.read()
    # if we don&#39;t read the image successfully break the loop
    if not success:
        break

    img = detector.findHands(frame)
    landmark_list = detector.findPosition(img)
    if len(landmark_list) != 0:
        thumb_x, thumb_y = landmark_list[4][1], landmark_list[4][2]
        index_x, index_y = landmark_list[8][1], landmark_list[8][2]
        # Middle point of the line connecting the thumb and index fingers
        cx, cy = (thumb_x + index_x) // 2, (thumb_y + index_y) // 2

        cv2.circle(img, (thumb_x, thumb_y), 10, (255, 0, 255), cv2.FILLED)
        cv2.circle(img, (index_x, index_y), 10, (255, 0, 255), cv2.FILLED)
        cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)
        cv2.line(img, (thumb_x, thumb_y), (index_x, index_y), (255, 0, 255), 3)

        # Check the legnth of the line to define the UPPER and LOWER bounds
        # Then when we are in the UPPER bound = 100% Volumne
        line_length = np.hypot(index_x - thumb_x, index_y - thumb_y)

        # Map range [30, 320] to [MIN_VOLUME, MAX_VOLUME]
        vol = np.interp(line_length, [LOWER_BOUND, UPPER_BOUND], [MIN_VOLUME, MAX_VOLUME])
        volume.SetMasterVolumeLevel(vol, None)

        vol_bar = np.interp(line_length, [LOWER_BOUND, UPPER_BOUND], [400, 150])
        vol_per = np.interp(line_length, [LOWER_BOUND, UPPER_BOUND], [0, 100])

        if line_length &amp;lt; LOWER_BOUND:
            cv2.circle(img, (cx, cy), 15, (0, 255, 0), cv2.FILLED)


    cv2.rectangle(img, (50, 150), (85, 400), (0, 255, 0), 3)
    cv2.rectangle(img, (50, int(vol_bar)), (85, 400), (0, 255, 0), cv2.FILLED)
    cv2.putText(img, f&amp;quot;{int(vol_per)}&amp;quot;, (40, 450), cv2.FONT_ITALIC, 1, (0, 255, 0), 2)

    current_time = time.time()
    fps = int(1 / (current_time - previous_time))
    previous_time = current_time
    cv2.putText(img, f&amp;quot;FPS: {fps}&amp;quot;, (40, 50), cv2.FONT_ITALIC, 1, (255, 0, 255), 2)

    cv2.imshow(win_name, frame)

# Since we exit the loop, its time to clean resources
source.release()
cv2.destroyWindow(win_name)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Conic Optimization on Julia</title>
      <link>https://danpereda.github.io/post/conicopt/</link>
      <pubDate>Fri, 07 Aug 2020 17:23:25 -0400</pubDate>
      <guid>https://danpereda.github.io/post/conicopt/</guid>
      <description>&lt;p&gt;Here I will describe a bit about conic programming on Julia based on 
&lt;a href=&#34;https://youtu.be/0ZCCWzGGjcY?t=2825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Juan Pablo Vielma&amp;rsquo;s JuliaCon 2020 talk&lt;/a&gt; and 

&lt;a href=&#34;https://nbviewer.jupyter.org/github/jump-dev/JuMPTutorials.jl/blob/master/notebook/optimization_concepts/conic_programming.ipynb#Conic-Programming&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JuMP devs Tutorials&lt;/a&gt;. We will begin by defining what is a cone and how to model them on &lt;code&gt;JuMP&lt;/code&gt; together with some simple examples, by the end we will solve an mixed - integer conic problem of avoiding obstacles by following a polynomial trajectory.&lt;/p&gt;
&lt;p&gt;Why Conic Optimizacion?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear- programming-like duality&lt;/li&gt;
&lt;li&gt;Faster and more stable algorithms
&lt;ul&gt;
&lt;li&gt;Avoid non-differentiability issues, exploit primal-dual form, strong theory on barriers for interior point algorithms.&lt;/li&gt;
&lt;li&gt;Industry change in 2018:
&lt;ol&gt;
&lt;li&gt;Knitro version 11.0 adds support for SOCP constraints.&lt;/li&gt;
&lt;li&gt;Mosek version 9.0 deprecates expression/function-based formulations and focuses on pure conic (linear, SOCP, rotated SOCP, SDP, exp &amp;amp; power)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;What is a Cone?&lt;/li&gt;
&lt;li&gt;Conic Programming.&lt;/li&gt;
&lt;li&gt;Some type of Cones supported by JuMP and programming examples.
&lt;ul&gt;
&lt;li&gt;Second Order - Cone&lt;/li&gt;
&lt;li&gt;Rotated Second Order - Cone&lt;/li&gt;
&lt;li&gt;Exponential Cone&lt;/li&gt;
&lt;li&gt;Positive Semidefinite Cone (PSD)&lt;/li&gt;
&lt;li&gt;Other Cones and Functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mixed Integer Conic example: Avoiding obstacles (Drone and Flappy bird)&lt;/li&gt;
&lt;li&gt;Continuous Conic programming on Julia?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-a-cone&#34;&gt;What is a Cone?&lt;/h2&gt;
&lt;p&gt;A subset $C$ of a vector space $V$ is a cone if $\forall x \in C$ and positive scalars $\alpha$, the product $\alpha x \in C$.&lt;/p&gt;
&lt;p&gt;A cone C is a convex cone if $\alpha x + \beta y \in C$, for any positive scalars $\alpha, \beta$, and any $x, y \in C$.&amp;quot;&lt;/p&gt;
&lt;h2 id=&#34;conic-programming&#34;&gt;Conic Programming&lt;/h2&gt;
&lt;p&gt;Conic programming problems are convex optimization problems in which a convex function is minimized over the intersection of an affine subspace and a convex cone. An example of a conic-form minimization problems, in the primal form is:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\min_{ x \in \mathbb{R}^n} a_0 ^\top x + b_0
\end{equation}&lt;/p&gt;
&lt;p&gt;such that:
$$A_i x + b_i \in \mathcal{C}_i  \quad \text{for } i = 1 \dotso m$$&lt;/p&gt;
&lt;p&gt;The corresponding dual problem is:&lt;/p&gt;
&lt;p&gt;$$ \max_{y_1, \dotso , y_m} - \sum_{i = 1}^{m} b_i ^T y_i + b_0$$&lt;/p&gt;
&lt;p&gt;such that:
$$ a_0 - \sum_{i = 1}^{m} A_{i}^{T} y_{i} = 0 $$
$$ y_i \in \mathcal{C}_i^*$$&lt;/p&gt;
&lt;p&gt;Where each $\mathcal{C}_i$ is a closed convex cone and $\mathcal{C}_i^*$ is its dual cone.&lt;/p&gt;
&lt;h2 id=&#34;some-of-the-types-of-cones-supported-by-jump&#34;&gt;Some of the Types of Cones supported by JuMP&lt;/h2&gt;
&lt;h3 id=&#34;second---order-cone&#34;&gt;Second - Order Cone&lt;/h3&gt;
&lt;p&gt;The Second - Order Cone (or Lorentz Cone) of dimension $n$ is of the form:&lt;/p&gt;
&lt;p&gt;$$ Q^n = \{ (t,x) \in \mathbb{R}^n: t \ge \lVert x \rVert_2 \} $$&lt;/p&gt;
&lt;p&gt;A Second - Order Cone rotated by $\pi/4$ in the $(x_1,x_2)$ plane is called a Rotated Second- Order Cone. It is of the form:&lt;/p&gt;
&lt;p&gt;$$ Q^n_r = \{ (t,u,x) \in \mathbb{R}^n: 2tu \ge \lVert x \rVert_2, t, u \ge 0 \} $$&lt;/p&gt;
&lt;p&gt;These cones are represented in &lt;code&gt;JuMP&lt;/code&gt; using &lt;code&gt;MOI&lt;/code&gt; sets &lt;code&gt;SecondOrderCone&lt;/code&gt; and &lt;code&gt;RotatedSecondOrderCone&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;example-euclidean-projection-on-a-hyperplane&#34;&gt;Example: Euclidean Projection on a hyperplane&lt;/h4&gt;
&lt;p&gt;For a given point $u_0$ and a set $K$, we refer to any point $u \in K$ which is closest to $u_0$ as a projection of $u_0$ on $K$. The projection of a point $u_0$ on a hyperplane $K = \{ u : p&#39;\cdot u = q \}$ is given by:&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \min_{x \in \mathbb{R}^n} &amp;amp; \lVert u - u_0 \rVert \\&lt;br&gt;
&amp;amp; \text{s.t. } &amp;amp; p&#39;\cdot u = q 
\end{align}$$&lt;/p&gt;
&lt;p&gt;We can model the above problem as the following conic program:&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \min &amp;amp; t \\&lt;br&gt;
&amp;amp; \text{s.t. } &amp;amp; p&#39;\cdot u = q \\&lt;br&gt;
&amp;amp;  \quad &amp;amp; (t, u - u_0) \in Q^{n+1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;If we transform this to the form we saw above,&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
x &amp;amp; = (t,u)\\&lt;br&gt;
a_0 &amp;amp; = e_1\\&lt;br&gt;
b_0 &amp;amp; = 0\\&lt;br&gt;
A_1 &amp;amp; =(0,p)\\&lt;br&gt;
b_1 &amp;amp; = -q \\&lt;br&gt;
C_1 &amp;amp; = \mathbb{R}\\&lt;br&gt;
A_2 &amp;amp; = 1\\&lt;br&gt;
b_2 &amp;amp;= -(0,u_0)\\&lt;br&gt;
C_2 &amp;amp;= Q^{n+1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Thus, we can obtain the dual problem as:&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \max &amp;amp; y_1 + (0,u_0)^\top y_2 \\&lt;br&gt;
&amp;amp; \text{s.t. } &amp;amp; e_1 -(0,p)^\top y_1 - y_2 = 0 \\&lt;br&gt;
&amp;amp;  \quad &amp;amp; y_1 \in \mathbb{R}\\&lt;br&gt;
&amp;amp; \quad &amp;amp; y_2 \in Q^{n+1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s model this in Julia.&lt;/p&gt;
&lt;p&gt;First we need to load some packages &lt;code&gt;JuMP&lt;/code&gt; is the modeling package, &lt;code&gt;ECOS&lt;/code&gt; is a solver, &lt;code&gt;LinearAlgebra&lt;/code&gt; and &lt;code&gt;Random&lt;/code&gt; are just to get some linear algebra operations and a fix seed for reproducibility respectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using JuMP
using ECOS
using LinearAlgebra
using Random
Random.seed!(2020);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets get some random values for the problem&amp;rsquo;s input:  $u_0$, $p$ and $q$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;u0 = rand(10)
p = rand(10)
q = rand();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can write the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;model = Model(optimizer_with_attributes(ECOS.Optimizer, &amp;quot;printlevel&amp;quot; =&amp;gt; 0))
@variable(model, u[1:10])
@variable(model, t)
@objective(model, Min, t)
@constraint(model, [t, (u - u0)...] in SecondOrderCone())
@constraint(model, u&#39; * p == q)
optimize!(model)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-optimization-result&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrder.png&#34; data-caption=&#34;Optimization Result&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrder.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Optimization Result
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Then we can see the objective function value and variable value at the optimum by doing:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@show objective_value(model);
@show value.(u);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an objective value of : $1.4149915748070703$. We can also solve the dual problem:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;e1 = [1.0, zeros(10)...]
dual_model = Model(optimizer_with_attributes(ECOS.Optimizer, &amp;quot;printlevel&amp;quot; =&amp;gt; 0))
@variable(dual_model, y1 &amp;lt;= 0.0)
@variable(dual_model, y2[1:11])
@objective(dual_model, Max, q * y1 + dot(vcat(0.0, u0), y2))
@constraint(dual_model, e1 - [0.0, p...] .* y1 - y2 .== 0.0)
@constraint(dual_model, y2 in SecondOrderCone())
optimize!(dual_model)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-optimization-dual-problem&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrderDual.png&#34; data-caption=&#34;Optimization Dual problem&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrderDual.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Optimization Dual problem
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@show objective_value(dual_model);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an objective value of : $1.4149916455792486$. The difference between this value and the primal is $ \approx 7.07 \times 10^{-8}$, does this makes sense?&lt;/p&gt;
&lt;p&gt;We can also have an equivalent formulation using a Rotated Second - Order cone:&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \min &amp;amp; t \\&lt;br&gt;
&amp;amp; \text{s.t. } &amp;amp; p&#39;\cdot u = q \\&lt;br&gt;
&amp;amp;  \quad &amp;amp; (t,  1/2, u - u_0) \in Q^{n+2}_r
\end{align}$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;model = Model(optimizer_with_attributes(ECOS.Optimizer, &amp;quot;printlevel&amp;quot; =&amp;gt; 0))
@variable(model, u[1:10])
@variable(model, t)
@objective(model, Min, t)
@constraint(model, [t, 0.5, (u - u0)...] in RotatedSecondOrderCone())
@constraint(model, u&#39; * p == q)
optimize!(model)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-optimization-rotated-formulation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrderRotated.png&#34; data-caption=&#34;Optimization Rotated formulation&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/ConicOp/2ndOrderRotated.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Optimization Rotated formulation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We notice that the objective function values are different. There is a simple explanation to that behaviour. In the case of Second-Order Cone the objective function is $\lVert u - u_0 \rVert _2$ while in the case of a Rotated Second-Order Cone is $\lVert u - u_0 \rVert_2^2$. However, &lt;strong&gt;the values of $u$&lt;/strong&gt; are the same in both problems.&lt;/p&gt;
&lt;h3 id=&#34;exponential-cone&#34;&gt;Exponential Cone&lt;/h3&gt;
&lt;p&gt;An exponential Cone is a set of the form:&lt;/p&gt;
&lt;p&gt;$$ K_{\exp} = \{  (x,y,z) \in \mathbb{R}^3 : y \cdot \exp(x/y) \le z, y \ge 0 \}$$&lt;/p&gt;
&lt;p&gt;It is represented in &lt;code&gt;JuMP&lt;/code&gt; using the &lt;code&gt;MOI&lt;/code&gt; set &lt;code&gt;ExponentialCone&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;example-entropy-maximization&#34;&gt;Example: Entropy Maximization&lt;/h4&gt;
&lt;p&gt;We want to maximize the entropy function $ H(x) = - x log (x)$ subject to linear inequality constraints.&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \max &amp;amp; -\sum_{i=1}^{n} x_ilog(x_i) \\&lt;br&gt;
&amp;amp; \text{s.t. } &amp;amp; \mathbf{1}^\top x = 1\\&lt;br&gt;
&amp;amp;  \quad &amp;amp; Ax \le b
\end{align}$$&lt;/p&gt;
&lt;p&gt;We just need to use the following transformation:&lt;/p&gt;
&lt;p&gt;$$ t \le -xlog(x) \iff t \le x log(1/x) \iff (t,x,1) \in K_{\exp}$$&lt;/p&gt;
&lt;p&gt;An example in Julia would be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;n = 15;
m = 10;
A = randn(m, n);
b = rand(m, 1);

model = Model(optimizer_with_attributes(ECOS.Optimizer, &amp;quot;printlevel&amp;quot; =&amp;gt; 0))
@variable(model, t[1:n])
@variable(model, x[1:n])
@objective(model, Max, sum(t))
@constraint(model, sum(x) == 1.0)
@constraint(model, A * x .&amp;lt;= b )
# Cannot use the exponential cone directly in JuMP, hence we use MOI to specify the set.
@constraint(model, con[i = 1:n], [t[i], x[i], 1.0] in MOI.ExponentialCone())

optimize!(model);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;positive-semidefinite-cone&#34;&gt;Positive Semidefinite Cone&lt;/h3&gt;
&lt;p&gt;The set of Positive Semidefinite Matrices of dimension $n$ form a cone in $\mathbb{R}^n$. We write this set mathematically as:&lt;/p&gt;
&lt;p&gt;$$ S^n_+ = \{  X \in S^n : z ^\top X z \ge 0 , \forall z \in \mathbb{R}^n\}$$&lt;/p&gt;
&lt;p&gt;A PSD cone is represented in JuMP using the MOI sets &lt;code&gt;PositiveSemidefiniteConeTriangle&lt;/code&gt; (for upper  triangle of a PSD matrix) and &lt;code&gt;PositiveSemidefiniteConeSquare&lt;/code&gt; (for a complete PSD matrix). However, it is preferable to use the &lt;code&gt;PSDCone&lt;/code&gt; shortcut as illustrated below.&lt;/p&gt;
&lt;h4 id=&#34;example-largest-eigenvalue-of-a-symmetrix-matrix&#34;&gt;Example: Largest Eigenvalue of a Symmetrix Matrix&lt;/h4&gt;
&lt;p&gt;Suppose $A$ has eigenvalues $\lambda_1 \ge \lambda_2 \ge \dotso \ge \lambda_n$. Then the matrix $tI - A$ has eigenvalues $ t - \lambda_1$, $t - \lambda_2$, $\dotso$, $t - \lambda_n$. Note that $t I - A$ is PSD exactly when all these eigenvalues are non-negative, and this happends for values $t \ge \lambda_1$. Thus, we can model the problem of fiding the largest eigenvalue of a symmetrix matrix as:&lt;/p&gt;
&lt;p&gt;$$ \begin{align}
&amp;amp; \lambda_1 = \max  t\\&lt;br&gt;
\text{s.t. } &amp;amp; tI - A  \succeq 0
\end{align}$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using LinearAlgebra
using SCS

A = [3 2 4;
     2 0 2;
     4 2 3]

model = Model(optimizer_with_attributes(SCS.Optimizer, &amp;quot;verbose&amp;quot; =&amp;gt; 0))
@variable(model, t)
@objective(model, Min, t)
@constraint(model, t .* Matrix{Float64}(I, 3, 3) - A in PSDCone())

optimize!(model)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which give us $\lambda_1 = 8$.&lt;/p&gt;
&lt;h3 id=&#34;other-cones-and-functions&#34;&gt;Other Cones and Functions&lt;/h3&gt;
&lt;p&gt;For other cones supported by JuMP, check out the 
&lt;a href=&#34;https://jump.dev/MathOptInterface.jl/dev/apimanual/index.html#Standard-form-problem-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MathOptInterface Manual&lt;/a&gt;. A good resource for learning more about functions which can be modelled using cones is the 
&lt;a href=&#34;https://docs.mosek.com/modeling-cookbook/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSEK Modeling Cookbook&lt;/a&gt;. Also 
&lt;a href=&#34;https://jump.dev/JuMP.jl/v0.19.0/installation/index.html#Getting-Solvers-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check this link to find out all the different solvers and their supported problem types&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;mixed---integer-conic-example&#34;&gt;Mixed - Integer Conic example&lt;/h2&gt;
&lt;p&gt;Suppose we have a drone which we want to fly avoiding obstacles, how can we model and compute the optimal trajectory?&lt;/p&gt;





  











&lt;figure id=&#34;figure-avoiding-obstacles&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/ConicOp/Avoiding.png&#34; data-caption=&#34;Avoiding Obstacles&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/ConicOp/Avoiding.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Avoiding Obstacles
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Let $(x(t),y(t))_{t \in [0,1]}$ represent the position at each time $t \in [0,1]$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: Discretize time intro intervals $0 =  T_1 &amp;lt; T_2 &amp;lt; \dotso &amp;lt; T_N = 1$ and then describe position by polynomials $\{ p_i : [T_i, T_{i+1}] - &amp;gt; \mathbb{R}^2\}_{i=1}^{N}$ such that:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$(x(t), y(t)) = p_i(t) \quad t \in [T_i,T_{i+1}] $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Step 2: &amp;ldquo;Safe polyhedrons&amp;rdquo; $P^r = \{ x \in \mathbb{R}^2 : A^r x \le b^r\}$ such that:
$$ \forall i, \exists r \text{ s.t } p_i(t) \in P^r \quad t \in [T_i, T_{i+1}] $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_i(t) \in P^r \implies q_{i,r}(t) \ge 0 \quad \forall t$&lt;/li&gt;
&lt;li&gt;Sum-of-Squares (SOS): $$ q_{i,r}(t) = \sum_{j}r_j^2 \text{ where } r_j(t) \text{ is a polynomial function}$$&lt;/li&gt;
&lt;li&gt;Boyund degree of polynomials: &lt;code&gt;SDP&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using &lt;code&gt;JuMP.jl&lt;/code&gt;, &lt;code&gt;PolyJuMP.jl&lt;/code&gt;, &lt;code&gt;SumOfSquares.jl&lt;/code&gt;, MI-SDP Solver &lt;code&gt;Pajarito.jl&lt;/code&gt; for a 9 region, 8 time steps problem, we get optimal &amp;ldquo;smoothness&amp;rdquo; in 651 seconds as shown in the picture above.&lt;/p&gt;
&lt;p&gt;While for 60 horizontal segments &amp;amp; obstacle every 5: Optimal &amp;ldquo;clicks&amp;rdquo; in 80 seconds.&lt;/p&gt;





  











&lt;figure id=&#34;figure-avoiding-obstacles&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/ConicOp/Flappy.png&#34; data-caption=&#34;Avoiding Obstacles&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/ConicOp/Flappy.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Avoiding Obstacles
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;
&lt;a href=&#34;https://github.com/juan-pablo-vielma/grid-science-2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check more here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;continuous-solver&#34;&gt;Continuous solver?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;Hypatia.jl&lt;/code&gt; solver: conic interior point algorithms and interfaces (Chris Coey, MIT)
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/2005.01136&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A homogeneous interior-point solver for non - symmetric cones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Versatility &amp;amp; performance = More Cones
&lt;ul&gt;
&lt;li&gt;Two dozen predefined &lt;strong&gt;standard&lt;/strong&gt; and &lt;em&gt;exotic&lt;/em&gt; cones: e.g SDP, Sum-of-Squares and &amp;ldquo;Matrix&amp;rdquo; Sum-of-Squares for convexity/shape constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Customizable: &amp;ldquo;Bring your own &lt;em&gt;barrier&lt;/em&gt;&amp;rdquo; = &amp;quot; Bring your own cone&amp;quot;&lt;/li&gt;
&lt;li&gt;Take advantage of &lt;em&gt;Natural formulations&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Take advantage of Julia: multi-precision arithmetic, abstract linear operators, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Modeling with new and nonsymmetric cones (Lea Kapelevich, MIT)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tulip.jl&lt;/code&gt;: An interior-point LP solver with abstract linear algebra (Mathieu Tanneau, Polytechnique Montréal)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://blegat.github.io/publications/#phd_thesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Set Programming with JuMP&lt;/a&gt; (Benoît Legat, UC Louvain)&lt;/li&gt;
&lt;li&gt;JuliaMoments (Tillmann Weisser, Los Alamos National Laboratory)
-Dual of Sum-of-Squares&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Scientific Machine Learning on Julia</title>
      <link>https://danpereda.github.io/post/scientificmachinelearning/</link>
      <pubDate>Fri, 31 Jul 2020 02:56:27 -0400</pubDate>
      <guid>https://danpereda.github.io/post/scientificmachinelearning/</guid>
      <description>&lt;p&gt;Here is what I&amp;rsquo;ve learned from the WorkShop 
&lt;a href=&#34;https://www.youtube.com/watch?v=QwVO0Xh2Hbg&amp;amp;list=LL&amp;amp;index=6&amp;amp;t=10515s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doing Scientific Machine Learning (SciML) With Julia&lt;/a&gt; from Chris Rackauckas. There is also an 
&lt;a href=&#34;https://github.com/mitmath/18337&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT course&lt;/a&gt; and 
&lt;a href=&#34;https://tutorials.sciml.ai/html/exercises/01-workshop_exercises.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workshop exercises&lt;/a&gt; (with solutions) by the same author about this subject that I&amp;rsquo;ve been checking out and strongly recomend.&lt;/p&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Modeling with Differential Equations
&lt;ul&gt;
&lt;li&gt;Differential Equations&lt;/li&gt;
&lt;li&gt;Stochastic Differential Equations&lt;/li&gt;
&lt;li&gt;Delayed Differential Equations&lt;/li&gt;
&lt;li&gt;Callbacks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Automated model discovery via universal differential equations
&lt;ul&gt;
&lt;li&gt;Parameter inference on differential equations
&lt;ol&gt;
&lt;li&gt;Local and global optimization&lt;/li&gt;
&lt;li&gt;Bayesian optimization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Neural Ordinary Differential Equations with &lt;code&gt;sciml_train&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;Solving for the Lokta - Volterra model with few data.&lt;/li&gt;
&lt;li&gt;Universal ODEs learn and extrapolate other dynamical behaviors&lt;/li&gt;
&lt;li&gt;Transforming a neural network fit into equations in sparsified from using SInDy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solving differential equations with neural networks (physics-informed neural networks)
&lt;ul&gt;
&lt;li&gt;Toy example&lt;/li&gt;
&lt;li&gt;Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;modeling-with-differential-equations&#34;&gt;Modeling with Differential Equations&lt;/h1&gt;
&lt;h2 id=&#34;differential-equations&#34;&gt;Differential Equations&lt;/h2&gt;
&lt;p&gt;First we will see how to define a differential Equation on Julia, for this we will use the Latka Volterra equation that modelates a population of rabbits and wolves.&lt;/p&gt;
&lt;p&gt;$$ \dfrac{dx}{dt} = \alpha x - \beta xy$$
$$ \dfrac{dy}{dt} = \delta xy - \gamma y$$&lt;/p&gt;
&lt;p&gt;Something that may be silly but I find really nice is that you can write special caracters like 🐰, 🐺, α, β, γ and δ.&lt;/p&gt;
&lt;p&gt;We just need to charge the package &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; and write our differential equation as a function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DifferentialEquations
function lotka_volterra!(du,u,p,t)
    🐰,🐺 = u
    α,β,γ,δ = p
    du[1] = d🐰 = α*🐰 - β*🐰*🐺
    du[2] = d🐺 = γ*🐰*🐺 - δ*🐺
end
u₀ = [1.0,1.0]
tspan = (0.0, 10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(lotka_volterra!,u₀,tspan,p)
sol = solve(prob)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-lokta--volterra-solution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/LaktaVolterra.svg&#34; data-caption=&#34;Lokta- Volterra Solution&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/LaktaVolterra.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Lokta- Volterra Solution
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-rabbit-vs-wolves&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/ConejoLobo.svg&#34; data-caption=&#34;Rabbit vs Wolves&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/ConejoLobo.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Rabbit vs Wolves
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Easy optimizations can be made, we can choose a better solver for the problem, stop saving everystep, etc.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Sundials # Charge CVODE_BDF() solver
sol = solve(prob, CVODE_BDF(), save_everystep=false, abstol=1e-8, reltol=1e-8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also change the parameters by using the remake function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;remake(prob, p =[1.2,0.8,2.5,0.8])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;stochastic-differential-equation&#34;&gt;Stochastic Differential Equation&lt;/h2&gt;
&lt;p&gt;Now we have a multiplicative noise, given by the terms $\sigma_i x_i dW_i$ where $dW_i$ is a random number whos standard deviation is $dt$.
$$ dx = (\alpha x - \beta xy)dt + \sigma_1 x dW_1 $$
$$ dy = (\delta xy - \gamma y)dt + \sigma_2 y dW_2$$&lt;/p&gt;
&lt;p&gt;In julia we just need create the multiplicative noise function and added to the previous problem by using &lt;code&gt;SDEProblem&lt;/code&gt; instead of &lt;code&gt;ODEProblem&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function multiplicative_noise!(du,u,p,t)
    🐰,🐺 = u
    du[1] = 0.3*🐰
    du[2] = 0.3*🐺
end
prob = SDEProblem(lotka_volterra!,multiplicative_noise!,u₀,tspan,p)
sol = solve(prob)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Solving only once would not be the best, since we have a randomness, thus we made use of another set of functions already implemented in &lt;code&gt;DifferentialEquations.jl&lt;/code&gt; called &lt;code&gt;Ensemble&lt;/code&gt;. Firstly we ensemble the problem, secondly we solve for a given number of trajectories (aditonal parameters like EnsembleThreads can be written in order to parelalize the problem and get aditional performance) and finally we do a summary of what happened.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;ensembleprob = EnsembleProblem(prob)
sol = solve(ensembleprob, SOSRI(), EnsembleThreads(), trajectories=1000)
summ = EnsembleSummary(sol)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-simple-summary&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/Summary.svg&#34; data-caption=&#34;Simple Summary&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/Summary.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Simple Summary
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;
&lt;a href=&#34;https://diffeq.sciml.ai/stable/features/ensemble/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here&lt;/a&gt; to see more about Ensemble&lt;/p&gt;
&lt;h2 id=&#34;delayed-differential-equations&#34;&gt;Delayed Differential Equations&lt;/h2&gt;
&lt;p&gt;The amount of growth happening at time $t$ is not due to the amount of rabbits at time $t$&lt;/p&gt;





  











&lt;figure id=&#34;figure-delayed-differential-equation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/Lag.svg&#34; data-caption=&#34;Delayed Differential Equation&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/Lag.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Delayed Differential Equation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;population-control&#34;&gt;Population Control&lt;/h2&gt;
&lt;p&gt;Example, whenever the amount of wolves is equal to $3$ then we are allow to kill one. The &lt;strong&gt;key&lt;/strong&gt; feature to do this &lt;code&gt;Callbacks&lt;/code&gt;. So whenever a condition happens, then it affects the dynamics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;🔥🐺_condition(u,t,integrator) = u[2] - 3
🔥🐺_affect!(integrator) = integrator.u[2] -=1
🔥🐺_cb = ContinuousCallback(🔥🐺_condition,🔥🐺_affect!)
sol = solve(prob, callback = 🔥🐺_cb)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-population-control&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/PopulationControl.svg&#34; data-caption=&#34;Population Control&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/PopulationControl.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Population Control
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Of course this is not the most realistic model, since we don&amp;rsquo;t instanstly kill a wolf each time.&lt;/p&gt;
&lt;h1 id=&#34;automated-model-discovery-via-universal-differential-equations&#34;&gt;Automated model discovery via universal differential equations&lt;/h1&gt;
&lt;h2 id=&#34;parameter-inference-on-differential-equations&#34;&gt;Parameter inference on differential equations&lt;/h2&gt;
&lt;p&gt;Our goal will be to find parameters that make the Lotka-Volterra solution the one we had on the first part, so we define our loss as the squared distance from our the real solution &lt;code&gt;dataset = Array(sol)&lt;/code&gt; with parameters $p$ given by $\alpha = 1.5$, $\beta = 1.0$, $\gamma = 3.0$ and $\delta = 1.0$. Note that when using &lt;code&gt;sciml_train&lt;/code&gt;, the first return is the loss value, and the other returns are sent to the callback for monitoring convergence.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function loss(p)
    tmp_prob = remake(prob, p = p)
    tmp_sol = solve(tmp_prob, saveat = 0.1)
    sum(abs2, Array(tmp_sol) - dataset), tmp_sol
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we use the sciml_train function to train the parameters using BFGS to arrive at parameters which optimize for our goal.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux
using Optim
pinit = [1.2,0.8,2.5,0.8]
res  = DiffEqFlux.sciml_train(loss, pinit, BFGS(), maxiters = 100)
p_final = res.minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sciml_train&lt;/code&gt; allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Flux
function plot_callback(p,l,tmp_sol)
    tmp_prob = remake(prob, p = p)
    tmp_sol = solve(tmp_prob, saveat = 0.1)
    fig = plot(tmp_sol)
    scatter!(fig, sol.t,dataset&#39;)
    display(fig)
    false
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s optimize the model and get a nice animation of what is happening in each iteration:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;res  = DiffEqFlux.sciml_train(loss, pinit, BFGS(), cb = plot_callback, maxiters=300)
p_final = res.minimizer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In just $1.745$ seconds and $8658251$ allocations: 361.20 MiB (counting plots) we get a loss function of $2.401364 \times 10^{-23}$ and parameters:
$$\alpha =  1.5000000000009583 \approx 1.5$$
$$\beta = 1.0000000000008002 \approx 1.0$$
$$\gamma =  3.0000000000005405 \approx 3.0$$
$$\delta = 0.9999999999995174 \approx 1.0$$&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/optimization_ode/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here&lt;/a&gt; to see more&lt;/p&gt;
&lt;p&gt;Notice that the election of &lt;code&gt;BFGS&lt;/code&gt; makes us converge quickier than using &lt;code&gt;ADAM&lt;/code&gt; (try this yourself). Usually &lt;code&gt;ADAM&lt;/code&gt; its pretty good for the first iterations to get local optima but then its better to change to &lt;code&gt;BFGS&lt;/code&gt; to do the final steps. Otherwise we can use &lt;code&gt;BlackBoxOptim&lt;/code&gt; to get global optima algorithms.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using BlackBoxOptim
res  = DiffEqFlux.sciml_train(loss, pinit, 
                                    DiffEqFlux.BBO(), 
                                    cb = plot_callback,
                                    lower_bounds= 0.5ones(4),
                                    upper_bounds=4.0ones(4) )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After $48690$ steps we get best candidate found: $[1.5, 1.0, 3.0, 1.0]$&lt;/p&gt;
&lt;h2 id=&#34;bayesian-inference&#34;&gt;Bayesian Inference&lt;/h2&gt;
&lt;p&gt;In this section we will use &lt;code&gt;Turing.jl&lt;/code&gt; together with the documentation 
&lt;a href=&#34;https://turing.ml/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here to see more&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most of the scientific community deals with the basic problem of trying to mathematically model the reality around them and this often involves dynamical systems. The general trend to model these complex dynamical systems is through the use of differential equations. Differential equation models often have non-measurable parameters. The popular “forward-problem” of simulation consists of solving the differential equations for a given set of parameters, the “inverse problem” to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters. Bayesian inference provides a robust approach to parameter estimation with quantified uncertainty.&lt;/p&gt;
&lt;p&gt;First we set up all the packages that will be use together with a fix seed for reproducibility of the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Turing, Distributions, DataFrames, DifferentialEquations, DiffEqSensitivity

# Import MCMCChain, Plots, and StatsPlots for visualizations and diagnostics.
using MCMCChains, Plots, StatsPlots

# Set a seed for reproducibility.
using Random
Random.seed!(12);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will keep using the same Lotka-Volerra equation and we’ll generate the data to use for the parameter estimation from simulation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;odedata = Array(solve(prob,Tsit5(),saveat=0.1))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turing and DifferentialEquations are completely composable and you can write of the differential equation inside a Turing &lt;code&gt;@model&lt;/code&gt; and it will just work.&lt;/p&gt;
&lt;p&gt;We can rewrite the Lotka Volterra parameter estimation problem with a Turing &lt;code&gt;@model&lt;/code&gt; interface as below&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Turing.setadbackend(:forward_diff) #Small Model
@model function fitlv(data)
    σ ~ InverseGamma(2, 3)
    α ~ truncated(Normal(1.5,0.5),0.5,2.5)
    β ~ truncated(Normal(1.2,0.5),0,2)
    γ ~ truncated(Normal(3.0,0.5),1,4)
    δ ~ truncated(Normal(1.0,0.5),0,2)

    p = [α,β,γ,δ]
    prob = ODEProblem(lokta_volterra!,u₀,tspan,p)
    predicted = solve(prob,Tsit5(),saveat=0.1)

    for i = 1:length(predicted)
        data[:,i] ~ MvNormal(predicted[i], σ) #Maximum Likehood Estimation
    end
end
model = fitlv(odedata)
chain = sample(model, NUTS(.65),10000)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just give our prior distribution and solve the dynamics to calcule our predictions and then compare it with the data in a maximum likehood estimation.&lt;/p&gt;
&lt;h2 id=&#34;neural-ordinary-differential-equations-with-sciml_train&#34;&gt;Neural Ordinary Differential Equations with sciml_train&lt;/h2&gt;
&lt;p&gt;First, lets generate the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots
u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 1.5f0)
tsteps = range(tspan[1], tspan[2], length = datasize)
function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)&#39;true_A)&#39;
end
prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we create a neural network that represents some idea we know about the system.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;dudt2 = FastChain((x, p) -&amp;gt; x.^3,
                  FastDense(2, 50, tanh),
                  FastDense(50, 2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What the neural network is just a mathematical function with the right parameters, what the code is doing is just writting:&lt;/p&gt;
&lt;p&gt;$ Wx^3 \rightarrow Wx^3 + b \rightarrow tanh(Wx^3+b)$  $\rightarrow W_2 tanh(Wx^3+b) \rightarrow W_2 tanh(Wx^3+b) + b_2 $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For large neural networks its recommended to use &lt;code&gt;Flux&lt;/code&gt; instead of &lt;code&gt;DiffEqFlux&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can write the &lt;code&gt;ODEProblem&lt;/code&gt; and solve it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;neural_ode_f(u,p,t) = dudt2(u,p)
pinit = initial_params(dudt2)
prob = ODEProblem(neural_ode_f,u0,(0.0f0,1.5f0),pinit)
sol = solve(prob, saveat = tsteps)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-neural-ode&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/NeuralODE1.svg&#34; data-caption=&#34;Neural ODE&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/NeuralODE1.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Neural ODE
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As we see the initial guess is not good, since we just try it to approximate by a random &lt;code&gt;ODE&lt;/code&gt;. Then what we need to do its find the right parameters that describe the neural network such that matches the ODE well enough. Therefore we can do it as before:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function loss(p)
    tmp_prob = remake(prob,p=p)
    tmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)
    sum(abs2, Array(tmp_sol) - ode_data)
end

function neuralode_callback(p,l)
    tmp_prob = remake(prob,p=p)
    tmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)
    fig = plot(tmp_sol)
    scatter!(fig,tsteps,ode_data&#39;) 
    display(fig)
    false
end

DiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),
                                    cb = neuralode_callback,
                                    maxiters = 500)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a loss value of $0.0678$. This can be optimize by using &lt;code&gt;ADAM&lt;/code&gt; and then &lt;code&gt;BFGS&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;res = DiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),
                                    cb = neuralode_callback,
                                    maxiters = 100)
DiffEqFlux.sciml_train(loss, res.minimizer, 
                             BFGS(initial_stepnorm=0.01),
                             maxiters = 100,
                             cb = neuralode_callback)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we get a loss value of $ 1.591519 \times 10^{-3}$.&lt;/p&gt;
&lt;p&gt;We can see that most of the computational time is on the gradients. For instance &lt;code&gt;Zygote.jl&lt;/code&gt; and &lt;code&gt;Turing.jl&lt;/code&gt; take control over which algorithm is used in order to optimize performance, anyways we can always choose which one we want 
&lt;a href=&#34;https://diffeq.sciml.ai/stable/analysis/sensitivity/#Sensitivity-Algorithms-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see DifferentialEquations.jl&lt;/a&gt; documentation on &lt;code&gt;Sensitivity Algorithms&lt;/code&gt; for this matter.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://diffeqflux.sciml.ai/dev/examples/neural_ode_sciml/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here&lt;/a&gt; to check more about Neural ODE on the SciML ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;universal-odes-learn-and-extrapolate-other-dynamical-behaviors&#34;&gt;Universal ODEs learn and extrapolate other dynamical behaviors&lt;/h2&gt;
&lt;p&gt;Truth equation:&lt;/p&gt;
&lt;p&gt;$$ \dot{x} = \alpha x - \beta xy$$
$$ \dot{y} = \gamma xy - \delta y$$&lt;/p&gt;
&lt;p&gt;Partially-known neural embedded equations&lt;/p&gt;
&lt;p&gt;$$ \dot{x} = \alpha x - U_1(x,y)$$
$$ \dot{y} = -\delta y + U_2(x,y)$$&lt;/p&gt;
&lt;p&gt;Automatically recover the long-term behaviour from less than half of a period in a cyclick time series!&lt;/p&gt;
&lt;p&gt;Turn neural networks back intro equations with &lt;code&gt;SInDy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s define the experimental parameter for the Lokta - Volterra equation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;tspan = (0.0f0,3.0f0)
u0 = Float32[0.44249296,4.6280594]
p_ = Float32[1.3, 0.9, 0.8, 1.8]
prob = ODEProblem(lotka, u0,tspan, p_)
solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-few-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/SmallData.svg&#34; data-caption=&#34;Few data&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/SmallData.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Few data
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Then we add noise to the data so that we do not overfit.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;X = Array(solution)
Xₙ = X + Float32(1e-3)*randn(eltype(X), size(X))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the neueral network which learns $L(x, y, y(t-\tau))$. Actually, we do not care about overfitting right now, since we want to extract the derivative information without numerical differentiation.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;L = FastChain(FastDense(2, 32, tanh),FastDense(32, 32, tanh), FastDense(32, 2))
p = initial_params(L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s define now the neural network given by:&lt;/p&gt;
&lt;p&gt;$$ \dot{x} = \alpha x - U_1(x,y)$$
$$ \dot{y} = -\delta y + U_2(x,y)$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function dudt_(u, p,t)
    x, y = u
    z = L(u,p)
    [p_[1]*x + z[1],
    -p_[4]*y + z[2]]
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So then when we solve&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;prob_nn = ODEProblem(dudt_,u0, tspan, p)
sol_nn = concrete_solve(prob_nn, Tsit5(), u0, p, saveat = solution.t)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The thick curves represent the real solution, as we see, we get a decent predictor for only the first second, afterwards the prediction for $u_1(t)$ its pretty bad, while the prediction for $u_2(t)$ it&amp;rsquo;s ok.&lt;/p&gt;





  











&lt;figure id=&#34;figure-bad-predictor&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/Badpredictor.svg&#34; data-caption=&#34;Bad predictor&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/Badpredictor.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bad predictor
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s improve now. For this we will do as before, we perfom a prediction and then compute a &lt;code&gt;loss&lt;/code&gt; function on the prediction to check how well are we fitting the data. Finally we create a &lt;code&gt;Callback&lt;/code&gt; that saves the losses each $50$ iterations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function predict(θ)
    Array(concrete_solve(prob_nn, Vern7(), u0, θ, saveat = solution.t,
                         abstol=1e-6, reltol=1e-6,
                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))
end

function loss(θ)
    pred = predict(θ)
    sum(abs2, Xₙ .- pred), pred 
end

const losses = []
callback(θ,l,pred) = begin
    push!(losses, l)
    if length(losses)%50==0
        println(&amp;quot;Current loss after $(length(losses)) iterations: $(losses[end])&amp;quot;)
    end
    false
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First train with &lt;code&gt;ADAM&lt;/code&gt; for better convergence adn then train with &lt;code&gt;BFGS&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;res1 = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 200)
res2 = DiffEqFlux.sciml_train(loss, res1.minimizer, BFGS(initial_stepnorm=0.01), 
                                                    cb=callback, 
                                                    maxiters = 10000)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-losses&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/losses.svg&#34; data-caption=&#34;losses&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/losses.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    losses
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Final training loss after $482$ iterations is  $2.74 \times 10^{-4}$ and the approximation fits the real solution really well.&lt;/p&gt;





  











&lt;figure id=&#34;figure-real-solution-vs-approximation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/fitting.svg&#34; data-caption=&#34;Real solution vs Approximation&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/fitting.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Real solution vs Approximation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Notice that we purposely made the real solution curve thicker so that its easier to see, otherwise both curves are superposed.&lt;/p&gt;
&lt;p&gt;We can also check the derivatives.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;DX = Array(solution(solution.t, Val{1}))
prob_nn2 = ODEProblem(dudt_,u0, tspan, res2.minimizer)
_sol = solve(prob_nn2, Tsit5())
DX_ = Array(_sol(solution.t, Val{1}))
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-derivatives&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/derivatives.svg&#34; data-caption=&#34;Derivatives&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/derivatives.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Derivatives
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Finally, we know that the real functions are $\beta xy$ and $\gamma xy$. Lets check the error plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Ideal data
L̄ = [-p_[2]*(X[1,:].*X[2,:])&#39;;p_[3]*(X[1,:].*X[2,:])&#39;]
# Neural network guess
L̂ = L(Xₙ,res2.minimizer)
&lt;/code&gt;&lt;/pre&gt;





  











&lt;figure id=&#34;figure-real-solution-vs-approximation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/error.svg&#34; data-caption=&#34;Real solution vs Approximation&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/error.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Real solution vs Approximation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;transforming-a-neural-network-fit-into-equations-in-sparsified-from-using-sindy&#34;&gt;Transforming a neural network fit into equations in sparsified from using SInDy&lt;/h2&gt;
&lt;p&gt;Now we want to use this nice fit and transformed back into equations. For this porpuse we&amp;rsquo;ll use &lt;code&gt;ModelingToolkit.jl&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will let the model know that we have $2$ variables and then create a basis that can approximate this functions by linear combinations of $sin(u_1)$ , $cos(u_1)$, $sin(u_2)$, $cos(u_2)$, $constant$, $u_1(t)^k$, $u_2(t)^k$ and $u_1(t)^k * u_2(t)^{5-k}$ with $k = 1&amp;hellip;5$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@variables u[1:2]
# Lots of polynomials
polys = Operation[1]
for i ∈ 1:5
    push!(polys, u[1]^i)
    push!(polys, u[2]^i)
    for j ∈ i:5
        if i != j
            push!(polys, (u[1]^i)*(u[2]^j))
            push!(polys, u[2]^i*u[1]^i)
        end
    end
end
# And some other stuff
h = [cos.(u)...; sin.(u)...; polys...]
basis = Basis(h, u)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will create an optimizer for the SINDY problem 
&lt;a href=&#34;https://datadriven.sciml.ai/dev/sparse_identification/sindy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check more about Sparse Identification of Nonlinear Dynamics&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Create an optimizer for the SINDY problem
opt = SR3()
# Create the thresholds which should be used in the search process
λ = exp10.(-7:0.1:3)
# Target function to choose the results from; x = L0 of coefficients and L2-Error of the model
f_target(x, w) = iszero(x[1]) ? Inf : norm(w.*x, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s see what happens if we want to use pure SINDY, meaning we have no pior information, only the data generated by our neural network,i.e, we took the values of the differential equation through the time series, we run it on the neural network giving us the output $\hat{L}$ and the $X$ is the input of the neural network, then we make SINDY transform this data into a system of equations using the basis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Ψ = SInDy(Xₙ[:, :], DX[:, :], basis, λ, opt = opt, maxiter = 10000,
                                                f_target = f_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result we get : $$du_1 = \beta sin(u_1) + \alpha cos(u_2) + \gamma u_1^2$$
$$du_2 = \delta u_2 $$&lt;/p&gt;
&lt;p&gt;i.e, we failed, but remember that we didn&amp;rsquo;t use any pior information.&lt;/p&gt;
&lt;p&gt;Test on ideal derivative data for unknown function (not available).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;Ψ = SInDy(Xₙ[:, 5:end], L̄[:, 5:end], basis, λ, opt = opt, maxiter = 10000,
                                                        f_target = f_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time we succeded as we recovered the missing terms of each equation.&lt;/p&gt;
&lt;p&gt;$$du_1 = \beta u_1u_2 $$
$$du_2 = \gamma u_1u_2 $$&lt;/p&gt;
&lt;p&gt;And we also succeded using derivative data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Test on uode derivative data
println(&amp;quot;SINDY on learned, partial, available data&amp;quot;)
Ψ = SInDy(Xₙ[:, 2:end], L̂[:, 2:end], basis, λ,  opt = opt, maxiter = 10000, 
                                                        normalize = true, 
                                                        denoise = true, 
                                                        f_target = f_target)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can extract the parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;p̂ = parameters(Ψ)
println(&amp;quot;First parameter guess : $(p̂)&amp;quot;)
unknown_sys = ODESystem(Ψ)
unknown_eq = ODEFunction(unknown_sys)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The equations are recovered, but the parameters may not be the best, we can start another sindy run to get closer to the ground truth.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# Just the equations
b = Basis((u, p, t)-&amp;gt;unknown_eq(u, [1.; 1.], t), u)
# Retune for better parameters -&amp;gt; we could also use DiffEqFlux or other parameter estimation tools here.
Ψf = SInDy(Xₙ[:, 2:end], L̂[:, 2:end], b, opt = STRRidge(0.01), maxiter = 100, convergence_error = 1e-18) # Succeed
println(Ψf)
p̂ = parameters(Ψf)
println(&amp;quot;Second parameter guess : $(p̂)&amp;quot;)
# Create function
recovered_sys = ODESystem(Ψf)
recovered_eq = ODEFunction(recovered_sys)
# Build a ODE for the estimated system
function dudt(du, u, p, t)
    # Add SInDy Term
    α, δ, β, γ = p
    z = recovered_eq(u, [β; γ], t)
    du[1] = α*u[1] + z[1]
    du[2] = -δ*u[2] + z[2]
end
# Create the approximated problem and solution
ps = [p_[[1,4]]; p̂]
approximate_prob = ODEProblem(dudt, u0, tspan, ps)
approximate_solution = solve(approximate_prob, Tsit5(), saveat = 0.01)
# Look at long term prediction
t_long = (0.0, 50.0)
approximate_prob = ODEProblem(dudt, u0, t_long, ps)
approximate_solution_long = solve(approximate_prob, Tsit5(), saveat = 0.1) # Using higher tolerances here results in exit of julia
true_prob = ODEProblem(lotka, u0, t_long, p_)
true_solution_long = solve(true_prob, Tsit5(), saveat = approximate_solution_long.t)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why are neural networks so good? because for large enough neural network, local optima are global optima.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/ChrisRackauckas/universal_differential_equations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here to see more about Universal differential equations&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;physics-informed-neural-networks&#34;&gt;Physics-Informed Neural Networks&lt;/h1&gt;
&lt;h2 id=&#34;solve-and-ode-with-a-neural-network&#34;&gt;Solve and ODE with a neural network&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Let $u&#39; = f(u,t)$ with $u(0) = u_0$. We want to build a neural network $NN(t)$ that is the solution to this differential equation.&lt;/li&gt;
&lt;li&gt;By definition then, we must have that $NN&#39;(t) = f(NN(t),t)$ and $NN(0) = u_0$.&lt;/li&gt;
&lt;li&gt;Define $\mathcal{C}(\theta) = \displaystyle \sum_{t} \lVert NN&#39;(t) - f(NN(t),t) \rVert$ for $\theta$ the parameters of the ODE.
&lt;ul&gt;
&lt;li&gt;Then this cost is zero when $NN(t)$ is the solution to the ODE&lt;/li&gt;
&lt;li&gt;Therefore we aim to minimize this loss to get the solution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Extra trick: We can use $g(t) = tNN(t) + u_0$ as a test function instead of $NN(t)$. Notice that it is an approximator that always satisfies the boundary condition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s solve the ODE: $$u&#39;(t) = cos(2\pi t) = f(u,t)$$ with initial condition $$u(0) = 1$$&lt;/p&gt;
&lt;p&gt;Following the steps above, we create a neural network, a function $g(t)$ which we derivate and compute the $l_2$ norm of the difference between $g&#39;(t)$ and $u&#39;(t)$. In order to do this, we define $\epsilon$ as small as the precision of a &lt;em&gt;Float32&lt;/em&gt; allow us.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using Flux
NNODE = Chain( x -&amp;gt; [x],
            Dense(1,32,tanh),
            Dense(32,1),
            first)
NNODE(1.0)
g(t) = t*NNODE(t) + 1f0

using Statistics
ϵ = sqrt(eps(Float32))
loss() = mean(abs2( ( ( g(t+ϵ) - g(t) )/ϵ ) - cos(2π*t) ) for t in 0f0:0.01f0:1f0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a callback and train the neural network using a descent algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;iter = 0
cb = function()
    global iter += 1
    if iter % 500 == 0
        display( loss() )
    end
end
opt = Flux.Descent(0.01) 
data = Iterators.repeated( (), 5000 )
Flux.train!(loss, Flux.params(NNODE), data, opt; cb= cb)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compare to the real solution:&lt;/p&gt;
&lt;p&gt;$$ u(t) = \int u&#39;(t) dt = \int cos(2\pi t)dt = \dfrac{sin(2\pi t)}{2\pi} + constant$$&lt;/p&gt;
&lt;p&gt;but $u(0) = 1$ therefore we get $u(t) = 1 + \dfrac{sin(2\pi t)}{2\pi}$&lt;/p&gt;





  











&lt;figure id=&#34;figure-true-solution-vs-neural-network&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SciML/PINN.svg&#34; data-caption=&#34;True Solution vs Neural Network&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SciML/PINN.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    True Solution vs Neural Network
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Why Physics- Informed Neural Networks?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{C}(\theta) = C_{pde}(\theta) + C_{boundary} + C_{data}(\theta)$ can nudge a model towards data.
&lt;ul&gt;
&lt;li&gt;Equivalent to regularizing the neural network by a scientific equation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can train fast continuous surrogates by making the neural network parameter dependent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;example-solving-a-100-dimensional-hamilton-jacobi-bellman-equation&#34;&gt;Example: Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation&lt;/h3&gt;
&lt;p&gt;For this problem we will use &lt;code&gt;NeuralPDE.jl&lt;/code&gt; (
&lt;a href=&#34;https://github.com/SciML/NeuralPDE.jl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check more here&lt;/a&gt;). Following this steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write the function and equation.&lt;/li&gt;
&lt;li&gt;Make $\sigma^\top \nabla u (t,X)$ a neural network.&lt;/li&gt;
&lt;li&gt;Solve the resulting SDEs and learn $\sigma^\top \nabla u$ via:
  &lt;!-- $$l(\theta) = \displaystyle \mathbb{E} \left[ | g(X_{t_N}) - \hat{u}(\{ X_{t_n}{_{0 \le n \le N}}\} , \{ W_{t_n}_{0 \le n \le N}\} )|^2 \right] $$ --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using NeuralPDE
using Flux
using DifferentialEquations
using LinearAlgebra
d = 100 # number of dimensions
X0 = fill(0.0f0, d) # initial value of stochastic control process
tspan = (0.0f0, 1.0f0)
λ = 1.0f0

g(X) = log(0.5f0 + 0.5f0 * sum(X.^2))
f(X,u,σᵀ∇u,p,t) = -λ * sum(σᵀ∇u.^2)
μ_f(X,p,t) = zero(X)  # Vector d x 1 λ
σ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d
prob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)
hls = 10 + d # hidden layer size
opt = Flux.ADAM(0.01)  # optimizer
# sub-neural network approximating solutions at the desired point
u0 = Flux.Chain(Dense(d, hls, relu),
                Dense(hls, hls, relu),
                Dense(hls, 1))
# sub-neural network approximating the spatial gradients at time point
σᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, d))
pdealg = NNPDENS(u0, σᵀ∇u, opt=opt)
@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,
                            alg=EM(), dt=1.2, pabstol=1f-2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can solve this high dimensional problem in only $22.623969$ seconds $523.95 M$ allocations: $36.683 GiB$, $17.95%$ gc time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learn Julia via epidemic modelling</title>
      <link>https://danpereda.github.io/post/test/</link>
      <pubDate>Tue, 28 Jul 2020 17:57:12 -0400</pubDate>
      <guid>https://danpereda.github.io/post/test/</guid>
      <description>&lt;p&gt;This is what I&amp;rsquo;ve learned from the 
&lt;a href=&#34;https://live.juliacon.org/talk/LSNEWV&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;workshop by David P. Sanders&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will &lt;strong&gt;simulate&lt;/strong&gt; the &lt;strong&gt;dynamics&lt;/strong&gt; of an epidemic, i.e, an outbreak of an infectious disease. In a population of people with &lt;strong&gt;N&lt;/strong&gt; individuals we will be interested in how the number of susceptible (&lt;strong&gt;S&lt;/strong&gt;),
infectious (&lt;strong&gt;I&lt;/strong&gt;) and recovered (&lt;strong&gt;R&lt;/strong&gt;) individual changes over time. We will begin by looking at simple models that take into account only total numbers of people, by the end of the workshop we should be able to structure a more complicated &lt;strong&gt;individual - based&lt;/strong&gt; or &lt;strong&gt;agent - based&lt;/strong&gt; simulation, where we model individual people moving around space and interacting with one another.&lt;/p&gt;
&lt;p&gt;For Simplicity, those individuals will be modelled as &lt;strong&gt;random walks&lt;/strong&gt; on a grid, i.e, points that choose a neighbouring grid point at random to jump to.&lt;/p&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Generic programming&lt;/li&gt;
&lt;li&gt;Composite types&lt;/li&gt;
&lt;li&gt;(Outer) constructors&lt;/li&gt;
&lt;li&gt;Generic programming with Types&lt;/li&gt;
&lt;li&gt;Types for agents&lt;/li&gt;
&lt;li&gt;Composition and Parametrised Types&lt;/li&gt;
&lt;li&gt;Spatial SIR model&lt;/li&gt;
&lt;li&gt;Dynamics&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generic-programming-random-walks&#34;&gt;Generic programming: Random walks&lt;/h2&gt;
&lt;p&gt;Each step roughly corresponds to a different function. Each different type of walker will need a different way to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;initialize()&lt;/code&gt; itself and then&lt;/li&gt;
&lt;li&gt;&lt;code&gt;move()&lt;/code&gt;  which will return the new position chosen by the walker.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore a walk of length T is given by the following function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function walk(T)
    pos = initialize()
    trajectory = [pos]    # make a Vector that contains just the current value of `pos`
    for t in 1:T
        new_pos = move(pos)
        push!(trajectory, new_pos)   # append to the Vector
        pos = new_pos     # update for next iteration
    end
    return trajectory
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We noticed that this depends on the functions &lt;code&gt;initialize()&lt;/code&gt; and &lt;code&gt;move()&lt;/code&gt; that should be defined on the global scope. Since a random walk can be in &lt;strong&gt;n&lt;/strong&gt; dimension, we would like to be able to run
the same function of all dimension, this is what is called generic programming.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function walk(initialize, move, T)
    pos = initialize()
    trajectory = [pos]
    for t in 1:T
        pos = move(pos)               # *update* the value pointed to by `pos`
        push!(trajectory, deepcopy(pos))  # at the cost of needing to copy `pos` when we store it if it is a vector
    end
    return trajectory
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This way we can have different &lt;code&gt;initialize()&lt;/code&gt; and &lt;code&gt;move()&lt;/code&gt; functions depending on the dimension of the walker and we will be able to recover the trajectory calling the same walk function.&lt;/p&gt;
&lt;p&gt;Now the question is, how can we efficiently store information about each walker? we would like to now not only the trajectory but also if he is susceptible, infected or recovered. This leads us to the following section.&lt;/p&gt;
&lt;h2 id=&#34;composite-types&#34;&gt;Composite types&lt;/h2&gt;
&lt;p&gt;The main &lt;strong&gt;idea&lt;/strong&gt; is to collect up or aggregate all relevant information into a new data structure, called a composite type (or custom type, aggregate type, user-defined type, &amp;hellip;).&lt;/p&gt;
&lt;p&gt;Basically we want to be able to specify the &amp;ldquo;template&amp;rdquo; / &amp;ldquo;shape&amp;rdquo; / &amp;ldquo;structure&amp;rdquo; for a bag or box that will contain all the relevant information; this specification is the type itself. Then we need to produce objects which have that structure, i.e. which contain the corresponding variables; these are called instances.&lt;/p&gt;
&lt;p&gt;In Julia this is accomplished using the struct keyword (short for &amp;ldquo;structure&amp;rdquo;). For example, we can make an object that contains the  $x$  and  $y$  coordinates of a walker in 2 dimensions as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Walker2D
    x::Int64
    y::Int64
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;outer-constructors&#34;&gt;(Outer) constructors&lt;/h3&gt;
&lt;p&gt;Suppose we want walkers to be born at the origin unless otherwise stated. We don&amp;rsquo;t want to have to write &lt;code&gt;Walker2D(0, 0)&lt;/code&gt; each time; we would like to just write &lt;code&gt;Walker2D()&lt;/code&gt;. In other words, we want to add a new method to the function Walker2D:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Walker2D() = Walker2D(0, 0)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Such a constructor is called an outer constructor, since it lives outside the definition of the type.&lt;/p&gt;
&lt;h3 id=&#34;making-walkers-move&#34;&gt;Making walkers move&lt;/h3&gt;
&lt;p&gt;We are not allowed to modify the fields of a walker because we defined the structure as being &lt;strong&gt;immutable&lt;/strong&gt; (if we want it to be &lt;strong&gt;mutable&lt;/strong&gt; we need to specify it). Usually this will give us better performance.
So in order to make our walker move, we need to create a &lt;em&gt;new object&lt;/em&gt; with the new position. This could seem expensive, but in fact the Julia compiler will often be able to completely remove this object creation and produce code that is just as efficient as if there were no object at all!&lt;/p&gt;
&lt;p&gt;Suppose we want to only move on the $ x - axis $ then we can just define:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;move(w::Walker2D) = Walker2D(w.x + rand( (-1,1) ), w.y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now supposed we need to defined a function that moves us to an adjacent point at random, then we can just throw a coin an choose a direction based on that result.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function jump(w::Walker2D)
    r = rand()
    if r &amp;gt; 0.5
        return Walker2D(w.x + rand( (-1,1) ), w.y)
    else
        return Walker2D(w.x, w.y + rand( (-1,1) ) )
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generic-programming-with-types&#34;&gt;Generic programming with Types&lt;/h3&gt;
&lt;p&gt;Before we create a walk function that depends on the functions &lt;code&gt;initialize()&lt;/code&gt; and &lt;code&gt;move()&lt;/code&gt;, but what if we just have one of each function with different methods? this solution should be better, otherwise we would have
to define functions &lt;code&gt;initialize_1D()&lt;/code&gt; and &lt;code&gt;initialize_2D()&lt;/code&gt; to pass it as an argument an make a distinction between 1 dimension and 2 dimension walkers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot;Calculate the trajectory of a walker `w` for time `T`.&amp;quot;
function walk(w, T)
    trajectory = [w]   # store the current (initial) position of `w`
    for t in 1:T
        w = move(w)    # update the value bound to `w`
        push!(trajectory, deepcopy(w))   # store the current value
    end

    return trajectory
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have not specified a type of $w$ this means that if we have a move function that works for instance for Integer numbers (BigInt, Int64 and so on) it should work and it should also work if we have a Walker2D
as an argument.&lt;/p&gt;
&lt;h2 id=&#34;types-for-agents&#34;&gt;Types for agents&lt;/h2&gt;
&lt;p&gt;We are getting towards our goal of putting everything together to make a model of people moving around and interacting with one another. Most people start off susceptible, but when an infectious person meets a susceptible the infection is transmitted with a certain probability.&lt;/p&gt;
&lt;p&gt;We will make an individual-based model, also called an agent-based model. We need a struct called Agent that contains whatever information an agent needs. In our case we will need a position and an infection status.&lt;/p&gt;
&lt;p&gt;The position will behave almost like a normal random walk that we have seen before, while the infection status needs to reflect whether the agent is susceptible (S), infectious (I) or recovered / removed (R).&lt;/p&gt;
&lt;h3 id=&#34;enums&#34;&gt;Enums&lt;/h3&gt;
&lt;p&gt;We could represent the infection status simply using an integer, e.g. 0, 1 or 2. But then our code will be hard to read, since we will be comparing the infection status to numbers all the time without remembering which one is which.&lt;/p&gt;
&lt;p&gt;A nice solution is just to use &lt;code&gt;@enums&lt;/code&gt; macro.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;@enum InfectionStatus S=1 I R   # specify that `S` corresponds to the value 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will have a &lt;em&gt;new Type&lt;/em&gt; InfectionStatus, with possible values &lt;strong&gt;S&lt;/strong&gt;, &lt;strong&gt;I&lt;/strong&gt; and &lt;strong&gt;R&lt;/strong&gt; that also store a numerical value $ S = 1 $, $ I = 2$ and $ R = 3$. Then we can do &lt;code&gt;Int(I)&lt;/code&gt; and it will return the integer 2, we can also do&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;status = I
if status == I
    println(&amp;quot;infected!&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and get &lt;strong&gt;infected!&lt;/strong&gt; as a result. This way the InfectionStatus information gets easy to manipulate and remember in our code.&lt;/p&gt;
&lt;h3 id=&#34;composition-and-parametrised-types&#34;&gt;Composition and Parametrised Types&lt;/h3&gt;
&lt;p&gt;We can place one object &lt;em&gt;inside&lt;/em&gt; another one.&lt;/p&gt;
&lt;p&gt;Suppose we have defined a SimpleWalker2D structure as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct SimpleWalker2D
    x::Int64
    y::Int64
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can define an Agent as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Agent
    position::SimpleWalker2D
    status::InfectionStatus
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can create an infected Agent in position $(1,2)$ by simply doing &lt;code&gt; w = SimpleWalker2D(1,2)&lt;/code&gt; and then &lt;code&gt;a = Agent(w,I)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As we learned before, we would like to have our program a bit more generic. One way of doing it is by parametrizing Types:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Agent{T}
    position::T
    status::InfectionStatus
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes beeing too generic can cause troubles if not careful. Then we can parametrise for only some Types. Suppose there is a common abstract type AbstractWalker for all of the possible types that we want to be able to use for T (this can be 1,2 and 3 dimension walkers for example), then we can write:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Agent{T &amp;lt;: AbstractWalker}
    position::T
    status::InfectionStatus
end
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;spatial-sir-model&#34;&gt;Spatial SIR model&lt;/h2&gt;
&lt;p&gt;Now we are ready to build the spatial model. It will consist of walkers moving in a 2D box.
This was an exercise left to the audience at the end of the talk, so we will solve it as it is written on the notebook.&lt;/p&gt;
&lt;h3 id=&#34;confinement-inside-a-box&#34;&gt;Confinement inside a box&lt;/h3&gt;
&lt;p&gt;We need agents to live inside a box so that they don&amp;rsquo;t disperse.&lt;/p&gt;
&lt;h4 id=&#34;exercise&#34;&gt;Exercise&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Make a &lt;code&gt;ConfinedWalker2D&lt;/code&gt; type. Its fields are a &lt;code&gt;Walker2D&lt;/code&gt; object and a box size, &lt;code&gt;L&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Walker2D
   x::Int64
   y::Int64
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct ConfinedWalker2D
    w::Walker2D
    L::Int64
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important part here is that we just give the size of the box as a parameter. We do not do an inner constructor
that checks if the position of the walker is inside the box. This is because inner constructors can be bothersome so we just need to keep
in mind that we should check boundaries at some future function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extend &lt;code&gt;move&lt;/code&gt; to &lt;code&gt;ConfinedWalker2D&lt;/code&gt;. If the walker tries to jump outside the box, i.e. outside the sites 1 to  𝐿 , in either direction, then it remains where it is.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function move(cw::ConfinedWalker2D)
    r = rand()
    step= rand([-1,1])
    if r &amp;gt; 0.5
        posx = cw.w.x + step
        posy = cw.w.y
    else
         posx = cw.w.x
         posy = cw.w.y + step
     end
    if (posx &amp;lt;= cw.L)&amp;amp;&amp;amp; (1 &amp;lt;= posx)&amp;amp;&amp;amp;(posy &amp;lt;= cw.L)&amp;amp;&amp;amp; (1 &amp;lt;= posy)
        w = Walker2D(posx,posy)
        return ConfinedWalker2D(w, cw.L)
    else
        return cw
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Make a confined &lt;code&gt;Agent&lt;/code&gt; and calculate and draw its trajectory to make sure it stays inside the box.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;struct Agent{T}
    cw::T
    status::InfectionStatus
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s consider $L = 6$ and initial position $(5,5)$&lt;/p&gt;





  











&lt;figure id=&#34;figure-trajectory&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/anim.gif&#34; data-caption=&#34;Trajectory&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/anim.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Trajectory
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We can see how it does not move outside the border and stays in the same position in the $16th$ move for example.&lt;/p&gt;
&lt;h2 id=&#34;initialization&#34;&gt;Initialization&lt;/h2&gt;
&lt;h3 id=&#34;exercises&#34;&gt;Exercises&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Write a function &lt;code&gt;initialize&lt;/code&gt; that takes parameters &lt;code&gt;𝐿&lt;/code&gt;, the box length, and &lt;code&gt;𝑁&lt;/code&gt;, the number of agents. It builds, one by one, a Vector of agents, by proposing a position for each one and checking if that position is already occupied. If it is occupied, it should generate another one, and so on until it finds a free spot. All of the agents should have state &lt;code&gt;S&lt;/code&gt;, except for one infectious individual (&lt;code&gt;I&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To do this you should write a function &lt;code&gt;check_occupied&lt;/code&gt; that checks if a particular position is occupied.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function check_ocupied(w::Walker2D,v)
    m = length(v)
    if m == 0
        return false
    else
        for i = 1:m
            if (w.x == v[i].cw.w.x) &amp;amp;&amp;amp; (w.y == v[i].cw.w.x)
                return true
            end
        end
        return false
    end
end
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function initialize(L,N)
    i= 0
    v = []
    while i &amp;lt; N
    x = rand(-L:L)
    y = rand(-L:L)
    w = Walker2D(x,y)
        if !check_ocupied(w,v)
            a = Agent( ConfinedWalker2D(w,L), S)
            push!(v,deepcopy(a))
            i = i+1
        end
    end
    index = rand(1:N)
    v[index] = Agent(v[index].position,I)
    return v
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Write a function  &lt;code&gt;visualize_agents&lt;/code&gt; that takes in a collection of agents as argument. It should plot a point for each agent, coloured according to its status&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;function visualize_agents(v)
    m = length(v)
    x = SA[zeros(m)]
    y = SA[zeros(m)]
    infection_status = []
    for i = 1:m
        x[1][i] = v[i].cw.w.x
        y[1][i] = v[i].cw.w.y
        push!(infection_status,deepcopy(Int(v[i].status)))
    end
    return scatter((x,y) , c = infection_status, ratio =1, leg = false)
end
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Run these functions to visualize the initial condition.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s consider $L = 6$ and $N = 20$. Then we get the following:&lt;/p&gt;





  











&lt;figure id=&#34;figure-initial-condition&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/VisualizeAgents.png&#34; data-caption=&#34;Initial condition&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/VisualizeAgents.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Initial condition
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;dynamics&#34;&gt;Dynamics&lt;/h2&gt;
&lt;p&gt;Now we just need to simulate the dynamics of the system. We will consider parameters $p_l$ and $p_R$, the probabilities of infection and recovery at each time step, respectively.
The system evolves as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First the system is initialized with only one random infected agent.&lt;/li&gt;
&lt;li&gt;Secondly, a single agent choosen randomly, call it $i$ tries to move to an adjancent position. If the position &lt;strong&gt;is&lt;/strong&gt; ocuppied, by agent $j$, then &lt;em&gt;neither&lt;/em&gt; of them move, but they interact as follows:&lt;/li&gt;
&lt;li&gt;If agent $i$ is infected and agent $j$ is susceptible then agent $j$ gets infected with probability $p_I$&lt;/li&gt;
&lt;li&gt;If agent $i$ is infected, it recovers with probability $p_R$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We do this part defining &lt;code&gt;step()&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;Notice that this model does not allow a recovered agent to get infected again.&lt;/p&gt;
&lt;p&gt;Then the step! function looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot;Receives a vector of agents and the required probabilities,
returns a modified agent vector&amp;quot;
function step!(v, pI, pR)
    n = length(v)
    i = rand(1:n)
    cwi = move(v[i].cw) # we move agent i 
    index = check_ocupied2(cwi.w,v) # Give us [] or the index j of agent
    m = length(index) # can be 0 or 1
    if m == 0
        v[i] = Agent(cwi, v[i].status)
    else
        for j in index
            v[i],v[j] = infection(v[i],v[j], pI, pR)
        end
    end
    return v
end
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Infection()&lt;/code&gt; function makes the interaction between the agents $i$ and $j$ following the previous rules.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how the system evolves after $1000$ steps. For this we will use $L = 6$ , $N = 30$, $p_I = 0.5$ and $p_R = 0.05$.
Orange means infected, Blue is susceptible and Green is recovered.&lt;/p&gt;





  











&lt;figure id=&#34;figure-system-evolution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/evolution-1000-0.5-0.05.gif&#34; data-caption=&#34;System Evolution&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/evolution-1000-0.5-0.05.gif&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    System Evolution
  &lt;/figcaption&gt;


&lt;/figure&gt;






  











&lt;figure id=&#34;figure-sir&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://danpereda.github.io/img/post/SIR.svg&#34; data-caption=&#34;SIR&#34;&gt;


  &lt;img src=&#34;https://danpereda.github.io/img/post/SIR.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    SIR
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h3&gt;
&lt;p&gt;This workshop and to be honest most of JuliaCon 2020 workshops were amazing, as you can learn so much. There is a known saying about Mathematicians being able to code and solve problems at the cost of doing a really long, slow and bad algorithm. Going to this workshops and trying to learn as much Julia as possible during JuliaCon is making me improve a lot in terms of coding and knowing the capabilities of the language, so I recomend checking the workshops videos on 
&lt;a href=&#34;https://www.youtube.com/c/TheJuliaLanguage&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;youtube&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
