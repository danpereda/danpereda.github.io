<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Daniel Pereda">

  
  
  
    
  
  <meta name="description" content="Here is what I&rsquo;ve learned from the WorkShop Doing Scientific Machine Learning (SciML) With Julia from Chris Rackauckas. There is also an MIT course and Workshop exercises (with solutions) by the same author about this subject that I&rsquo;ve been checking out and strongly recomend.">

  
  <link rel="alternate" hreflang="en-us" href="https://danpereda.github.io/post/scientificmachinelearning/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://danpereda.github.io/post/scientificmachinelearning/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Danpereda">
  <meta property="og:url" content="https://danpereda.github.io/post/scientificmachinelearning/">
  <meta property="og:title" content="Scientific Machine Learning on Julia | Danpereda">
  <meta property="og:description" content="Here is what I&rsquo;ve learned from the WorkShop Doing Scientific Machine Learning (SciML) With Julia from Chris Rackauckas. There is also an MIT course and Workshop exercises (with solutions) by the same author about this subject that I&rsquo;ve been checking out and strongly recomend."><meta property="og:image" content="https://danpereda.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://danpereda.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-07-31T02:56:27-04:00">
    
    <meta property="article:modified_time" content="2020-07-31T02:56:27-04:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://danpereda.github.io/post/scientificmachinelearning/"
  },
  "headline": "Scientific Machine Learning on Julia",
  
  "datePublished": "2020-07-31T02:56:27-04:00",
  "dateModified": "2020-07-31T02:56:27-04:00",
  
  "author": {
    "@type": "Person",
    "name": "Daniel Pereda"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Danpereda",
    "logo": {
      "@type": "ImageObject",
      "url": "https://danpereda.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Here is what I\u0026rsquo;ve learned from the WorkShop Doing Scientific Machine Learning (SciML) With Julia from Chris Rackauckas. There is also an MIT course and Workshop exercises (with solutions) by the same author about this subject that I\u0026rsquo;ve been checking out and strongly recomend."
}
</script>

  

  


  


  





  <title>Scientific Machine Learning on Julia | Danpereda</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Danpereda</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Danpereda</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Scientific Machine Learning on Julia</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jul 31, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    16 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Here is what I&rsquo;ve learned from the WorkShop 
<a href="https://www.youtube.com/watch?v=QwVO0Xh2Hbg&amp;list=LL&amp;index=6&amp;t=10515s" target="_blank" rel="noopener">Doing Scientific Machine Learning (SciML) With Julia</a> from Chris Rackauckas. There is also an 
<a href="https://github.com/mitmath/18337" target="_blank" rel="noopener">MIT course</a> and 
<a href="https://tutorials.sciml.ai/html/exercises/01-workshop_exercises.html" target="_blank" rel="noopener">Workshop exercises</a> (with solutions) by the same author about this subject that I&rsquo;ve been checking out and strongly recomend.</p>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li>Modeling with Differential Equations
<ul>
<li>Differential Equations</li>
<li>Stochastic Differential Equations</li>
<li>Delayed Differential Equations</li>
<li>Callbacks</li>
</ul>
</li>
<li>Automated model discovery via universal differential equations
<ul>
<li>Parameter inference on differential equations
<ol>
<li>Local and global optimization</li>
<li>Bayesian optimization</li>
</ol>
</li>
<li>Neural Ordinary Differential Equations with <code>sciml_train</code>
<ul>
<li>Solving for the Lokta - Volterra model with few data.</li>
<li>Universal ODEs learn and extrapolate other dynamical behaviors</li>
<li>Transforming a neural network fit into equations in sparsified from using SInDy</li>
</ul>
</li>
</ul>
</li>
<li>Solving differential equations with neural networks (physics-informed neural networks)
<ul>
<li>Toy example</li>
<li>Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation</li>
</ul>
</li>
</ul>
<h1 id="modeling-with-differential-equations">Modeling with Differential Equations</h1>
<h2 id="differential-equations">Differential Equations</h2>
<p>First we will see how to define a differential Equation on Julia, for this we will use the Latka Volterra equation that modelates a population of rabbits and wolves.</p>
<p>$$ \dfrac{dx}{dt} = \alpha x - \beta xy$$
$$ \dfrac{dy}{dt} = \delta xy - \gamma y$$</p>
<p>Something that may be silly but I find really nice is that you can write special caracters like üê∞, üê∫, Œ±, Œ≤, Œ≥ and Œ¥.</p>
<p>We just need to charge the package <code>DifferentialEquations.jl</code> and write our differential equation as a function.</p>
<pre><code class="language-julia">using DifferentialEquations
function lakta_volterra!(du,u,p,t)
    üê∞,üê∫ = u
    Œ±,Œ≤,Œ≥,Œ¥ = p
    du[1] = düê∞ = Œ±*üê∞ - Œ≤*üê∞*üê∫
    du[2] = düê∫ = Œ≥*üê∞*üê∫ - Œ¥*üê∫
end
u‚ÇÄ = [1.0,1.0]
tspan = (0.0, 10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(lakta_volterra!,u‚ÇÄ,tspan,p)
sol = solve(prob)
</code></pre>





  











<figure id="figure-lakta--volterra-solution">


  <a data-fancybox="" href="/img/post/SciML/LaktaVolterra.svg" data-caption="Lakta- Volterra Solution">


  <img src="/img/post/SciML/LaktaVolterra.svg" alt=""  >
</a>


  
  
  <figcaption>
    Lakta- Volterra Solution
  </figcaption>


</figure>






  











<figure id="figure-rabbit-vs-wolves">


  <a data-fancybox="" href="/img/post/SciML/ConejoLobo.svg" data-caption="Rabbit vs Wolves">


  <img src="/img/post/SciML/ConejoLobo.svg" alt=""  >
</a>


  
  
  <figcaption>
    Rabbit vs Wolves
  </figcaption>


</figure>

<p>Easy optimizations can be made, we can choose a better solver for the problem, stop saving everystep, etc.</p>
<pre><code class="language-julia">using Sundials # Charge CVODE_BDF() solver
sol = solve(prob, CVODE_BDF(), save_everystep=false, abstol=1e-8, reltol=1e-8)
</code></pre>
<p>We can also change the parameters by using the remake function.</p>
<pre><code class="language-julia">remake(prob, p =[1.2,0.8,2.5,0.8])
</code></pre>
<h2 id="stochastic-differential-equation">Stochastic Differential Equation</h2>
<p>Now we have a multiplicative noise, given by the terms $\sigma_i x_i dW_i$ where $dW_i$ is a random number whos standard deviation is $dt$.
$$ dx = (\alpha x - \beta xy)dt + \sigma_1 x dW_1 $$
$$ dy = (\delta xy - \gamma y)dt + \sigma_2 y dW_2$$</p>
<p>In julia we just need create the multiplicative noise function and added to the previous problem by using <code>SDEProblem</code> instead of <code>ODEProblem</code>.</p>
<pre><code class="language-julia">function multiplicative_noise!(du,u,p,t)
    üê∞,üê∫ = u
    du[1] = 0.3*üê∞
    du[2] = 0.3*üê∫
end
prob = SDEProblem(lakta_volterra!,multiplicative_noise!,u‚ÇÄ,tspan,p)
sol = solve(prob)
</code></pre>
<p>Solving only once would not be the best, since we have a randomness, thus we made use of another set of functions already implemented in <code>DifferentialEquations.jl</code> called <code>Ensemble</code>. Firstly we ensemble the problem, secondly we solve for a given number of trajectories (aditonal parameters like EnsembleThreads can be written in order to parelalize the problem and get aditional performance) and finally we do a summary of what happened.</p>
<pre><code class="language-julia">ensembleprob = EnsembleProblem(prob)
sol = solve(ensembleprob, SOSRI(), EnsembleThreads(), trajectories=1000)
summ = EnsembleSummary(sol)
</code></pre>





  











<figure id="figure-simple-summary">


  <a data-fancybox="" href="/img/post/SciML/Summary.svg" data-caption="Simple Summary">


  <img src="/img/post/SciML/Summary.svg" alt=""  >
</a>


  
  
  <figcaption>
    Simple Summary
  </figcaption>


</figure>

<p>
<a href="https://diffeq.sciml.ai/stable/features/ensemble/" target="_blank" rel="noopener">Click here</a> to see more about Ensemble</p>
<h2 id="delayed-differential-equations">Delayed Differential Equations</h2>
<p>The amount of growth happening at time $t$ is not due to the amount of rabbits at time $t$</p>





  











<figure id="figure-delayed-differential-equation">


  <a data-fancybox="" href="/img/post/SciML/Lag.svg" data-caption="Delayed Differential Equation">


  <img src="/img/post/SciML/Lag.svg" alt=""  >
</a>


  
  
  <figcaption>
    Delayed Differential Equation
  </figcaption>


</figure>

<h2 id="population-control">Population Control</h2>
<p>Example, whenever the amount of wolves is equal to $3$ then we are allow to kill one. The <strong>key</strong> feature to do this <code>Callbacks</code>. So whenever a condition happens, then it affects the dynamics.</p>
<pre><code class="language-julia">üî•üê∫_condition(u,t,integrator) = u[2] - 3
üî•üê∫_affect!(integrator) = integrator.u[2] -=1
üî•üê∫_cb = ContinuousCallback(üî•üê∫_condition,üî•üê∫_affect!)
sol = solve(prob, callback = üî•üê∫_cb)
</code></pre>





  











<figure id="figure-population-control">


  <a data-fancybox="" href="/img/post/SciML/PopulationControl.svg" data-caption="Population Control">


  <img src="/img/post/SciML/PopulationControl.svg" alt=""  >
</a>


  
  
  <figcaption>
    Population Control
  </figcaption>


</figure>

<p>Of course this is not the most realistic model, since we don&rsquo;t instanstly kill a wolf each time.</p>
<h1 id="automated-model-discovery-via-universal-differential-equations">Automated model discovery via universal differential equations</h1>
<h2 id="parameter-inference-on-differential-equations">Parameter inference on differential equations</h2>
<p>Our goal will be to find parameters that make the Lotka-Volterra solution the one we had on the first part, so we define our loss as the squared distance from our the real solution <code>dataset = Array(sol)</code> with parameters $p$ given by $\alpha = 1.5$, $\beta = 1.0$, $\gamma = 3.0$ and $\delta = 1.0$. Note that when using <code>sciml_train</code>, the first return is the loss value, and the other returns are sent to the callback for monitoring convergence.</p>
<pre><code class="language-julia">function loss(p)
    tmp_prob = remake(prob, p = p)
    tmp_sol = solve(tmp_prob, saveat = 0.1)
    sum(abs2, Array(tmp_sol) - dataset), tmp_sol
end
</code></pre>
<p>Lastly, we use the sciml_train function to train the parameters using BFGS to arrive at parameters which optimize for our goal.</p>
<pre><code class="language-julia">using DiffEqFlux
using Optim
pinit = [1.2,0.8,2.5,0.8]
res  = DiffEqFlux.sciml_train(loss, pinit, BFGS(), maxiters = 100)
p_final = res.minimizer
</code></pre>
<p><code>sciml_train</code> allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation.</p>
<pre><code class="language-julia">using Flux
function plot_callback(p,l,tmp_sol)
    tmp_prob = remake(prob, p = p)
    tmp_sol = solve(tmp_prob, saveat = 0.1)
    fig = plot(tmp_sol)
    scatter!(fig, sol.t,dataset')
    display(fig)
    false
end
</code></pre>
<p>Let&rsquo;s optimize the model and get a nice animation of what is happening in each iteration:</p>
<pre><code class="language-julia">res  = DiffEqFlux.sciml_train(loss, pinit, BFGS(), cb = plot_callback, maxiters=300)
p_final = res.minimizer
</code></pre>
<p>In just $1.745$ seconds and $8658251$ allocations: 361.20 MiB (counting plots) we get a loss function of $2.401364 \times 10^{-23}$ and parameters:
$$\alpha =  1.5000000000009583 \approx 1.5$$
$$\beta = 1.0000000000008002 \approx 1.0$$
$$\gamma =  3.0000000000005405 \approx 3.0$$ 
$$\delta = 0.9999999999995174 \approx 1.0$$</p>
<p>
<a href="https://diffeqflux.sciml.ai/dev/examples/optimization_ode/" target="_blank" rel="noopener">Click here</a> to see more</p>
<p>Notice that the election of <code>BFGS</code> makes us converge quickier than using <code>ADAM</code> (try this yourself). Usually <code>ADAM</code> its pretty good for the first iterations to get local optima but then its better to change to <code>BFGS</code> to do the final steps. Otherwise we can use <code>BlackBoxOptim</code> to get global optima algorithms.</p>
<pre><code class="language-julia">using BlackBoxOptim
res  = DiffEqFlux.sciml_train(loss, pinit, 
                                    DiffEqFlux.BBO(), 
                                    cb = plot_callback,
                                    lower_bounds= 0.5ones(4),
                                    upper_bounds=4.0ones(4) )

</code></pre>
<p>After $48690$ steps we get best candidate found: $[1.5, 1.0, 3.0, 1.0]$</p>
<h2 id="bayesian-inference">Bayesian Inference</h2>
<p>In this section we will use <code>Turing.jl</code> together with the documentation 
<a href="https://turing.ml/dev/" target="_blank" rel="noopener">Click here to see more</a>.</p>
<p>Most of the scientific community deals with the basic problem of trying to mathematically model the reality around them and this often involves dynamical systems. The general trend to model these complex dynamical systems is through the use of differential equations. Differential equation models often have non-measurable parameters. The popular ‚Äúforward-problem‚Äù of simulation consists of solving the differential equations for a given set of parameters, the ‚Äúinverse problem‚Äù to simulation, known as parameter estimation, is the process of utilizing data to determine these model parameters. Bayesian inference provides a robust approach to parameter estimation with quantified uncertainty.</p>
<p>First we set up all the packages that will be use together with a fix seed for reproducibility of the results.</p>
<pre><code class="language-julia">using Turing, Distributions, DataFrames, DifferentialEquations, DiffEqSensitivity

# Import MCMCChain, Plots, and StatsPlots for visualizations and diagnostics.
using MCMCChains, Plots, StatsPlots

# Set a seed for reproducibility.
using Random
Random.seed!(12);
</code></pre>
<p>We will keep using the same Lotka-Volerra equation and we‚Äôll generate the data to use for the parameter estimation from simulation.</p>
<pre><code class="language-julia">odedata = Array(solve(prob,Tsit5(),saveat=0.1))
</code></pre>
<p>Turing and DifferentialEquations are completely composable and you can write of the differential equation inside a Turing <code>@model</code> and it will just work.</p>
<p>We can rewrite the Lotka Volterra parameter estimation problem with a Turing <code>@model</code> interface as below</p>
<pre><code class="language-julia">Turing.setadbackend(:forward_diff) #Small Model
@model function fitlv(data)
    œÉ ~ InverseGamma(2, 3)
    Œ± ~ truncated(Normal(1.5,0.5),0.5,2.5)
    Œ≤ ~ truncated(Normal(1.2,0.5),0,2)
    Œ≥ ~ truncated(Normal(3.0,0.5),1,4)
    Œ¥ ~ truncated(Normal(1.0,0.5),0,2)

    p = [Œ±,Œ≤,Œ≥,Œ¥]
    prob = ODEProblem(lokta_volterra!,u‚ÇÄ,tspan,p)
    predicted = solve(prob,Tsit5(),saveat=0.1)

    for i = 1:length(predicted)
        data[:,i] ~ MvNormal(predicted[i], œÉ) #Maximum Likehood Estimation
    end
end
model = fitlv(odedata)
chain = sample(model, NUTS(.65),10000)
</code></pre>
<p>We just give our prior distribution and solve the dynamics to calcule our predictions and then compare it with the data in a maximum likehood estimation.</p>
<h2 id="neural-ordinary-differential-equations-with-sciml_train">Neural Ordinary Differential Equations with sciml_train</h2>
<p>First, lets generate the data</p>
<pre><code class="language-julia">using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots
u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 1.5f0)
tsteps = range(tspan[1], tspan[2], length = datasize)
function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u.^3)'true_A)'
end
prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))
</code></pre>
<p>Second, we create a neural network that represents some idea we know about the system.</p>
<pre><code class="language-julia">dudt2 = FastChain((x, p) -&gt; x.^3,
                  FastDense(2, 50, tanh),
                  FastDense(50, 2))
</code></pre>
<p>What the neural network is just a mathematical function with the right parameters, what the code is doing is just writting:</p>
<p>$ Wx^3 \rightarrow Wx^3 + b \rightarrow tanh(Wx^3+b)$  $\rightarrow W_2 tanh(Wx^3+b) \rightarrow W_2 tanh(Wx^3+b) + b_2 $</p>
<p><strong>Note</strong>: For large neural networks its recommended to use <code>Flux</code> instead of <code>DiffEqFlux</code>.</p>
<p>Now we can write the <code>ODEProblem</code> and solve it.</p>
<pre><code class="language-julia">neural_ode_f(u,p,t) = dudt2(u,p)
pinit = initial_params(dudt2)
prob = ODEProblem(neural_ode_f,u0,(0.0f0,1.5f0),pinit)
sol = solve(prob, saveat = tsteps)
</code></pre>





  











<figure id="figure-neural-ode">


  <a data-fancybox="" href="/img/post/SciML/NeuralODE1.svg" data-caption="Neural ODE">


  <img src="/img/post/SciML/NeuralODE1.svg" alt=""  >
</a>


  
  
  <figcaption>
    Neural ODE
  </figcaption>


</figure>

<p>As we see the initial guess is not good, since we just try it to approximate by a random <code>ODE</code>. Then what we need to do its find the right parameters that describe the neural network such that matches the ODE well enough. Therefore we can do it as before:</p>
<pre><code class="language-julia">function loss(p)
    tmp_prob = remake(prob,p=p)
    tmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)
    sum(abs2, Array(tmp_sol) - ode_data)
end

function neuralode_callback(p,l)
    tmp_prob = remake(prob,p=p)
    tmp_sol = solve(tmp_prob,Tsit5(), saveat = tsteps)
    fig = plot(tmp_sol)
    scatter!(fig,tsteps,ode_data') 
    display(fig)
    false
end

DiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),
                                    cb = neuralode_callback,
                                    maxiters = 500)

</code></pre>
<p>We get a loss value of $0.0678$. This can be optimize by using <code>ADAM</code> and then <code>BFGS</code></p>
<pre><code class="language-julia">res = DiffEqFlux.sciml_train(loss, pinit, ADAM(0.05),
                                    cb = neuralode_callback,
                                    maxiters = 100)
DiffEqFlux.sciml_train(loss, res.minimizer, 
                             BFGS(initial_stepnorm=0.01),
                             maxiters = 100,
                             cb = neuralode_callback)
</code></pre>
<p>Now we get a loss value of $ 1.591519 \times 10^{-3}$.</p>
<p>We can see that most of the computational time is on the gradients. For instance <code>Zygote.jl</code> and <code>Turing.jl</code> take control over which algorithm is used in order to optimize performance, anyways we can always choose which one we want 
<a href="https://diffeq.sciml.ai/stable/analysis/sensitivity/#Sensitivity-Algorithms-1" target="_blank" rel="noopener">see DifferentialEquations.jl</a> documentation on <code>Sensitivity Algorithms</code> for this matter.</p>
<p>
<a href="https://diffeqflux.sciml.ai/dev/examples/neural_ode_sciml/" target="_blank" rel="noopener">Click here</a> to check more about Neural ODE on the SciML ecosystem.</p>
<h2 id="universal-odes-learn-and-extrapolate-other-dynamical-behaviors">Universal ODEs learn and extrapolate other dynamical behaviors</h2>
<p>Truth equation:</p>
<p>$$ \dot{x} = \alpha x - \beta xy$$
$$ \dot{y} = \gamma xy - \delta y$$</p>
<p>Partially-known neural embedded equations</p>
<p>$$ \dot{x} = \alpha x - U_1(x,y)$$
$$ \dot{y} = -\delta y + U_2(x,y)$$</p>
<p>Automatically recover the long-term behaviour from less than half of a period in a cyclick time series!</p>
<p>Turn neural networks back intro equations with <code>SInDy</code>.</p>
<p>Let&rsquo;s define the experimental parameter for the Lokta - Volterra equation.</p>
<pre><code class="language-julia">tspan = (0.0f0,3.0f0)
u0 = Float32[0.44249296,4.6280594]
p_ = Float32[1.3, 0.9, 0.8, 1.8]
prob = ODEProblem(lotka, u0,tspan, p_)
solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)
</code></pre>





  











<figure id="figure-few-data">


  <a data-fancybox="" href="/img/post/SciML/SmallData.svg" data-caption="Few data">


  <img src="/img/post/SciML/SmallData.svg" alt=""  >
</a>


  
  
  <figcaption>
    Few data
  </figcaption>


</figure>

<p>Then we add noise to the data so that we do not overfit.</p>
<pre><code class="language-julia">X = Array(solution)
X‚Çô = X + Float32(1e-3)*randn(eltype(X), size(X))
</code></pre>
<p>Define the neueral network which learns $L(x, y, y(t-\tau))$. Actually, we do not care about overfitting right now, since we want to extract the derivative information without numerical differentiation.</p>
<pre><code class="language-julia">L = FastChain(FastDense(2, 32, tanh),FastDense(32, 32, tanh), FastDense(32, 2))
p = initial_params(L)
</code></pre>
<p>Let&rsquo;s define now the neural network given by:</p>
<p>$$ \dot{x} = \alpha x - U_1(x,y)$$
$$ \dot{y} = -\delta y + U_2(x,y)$$</p>
<pre><code class="language-julia">function dudt_(u, p,t)
    x, y = u
    z = L(u,p)
    [p_[1]*x + z[1],
    -p_[4]*y + z[2]]
end
</code></pre>
<p>So then when we solve</p>
<pre><code class="language-julia">prob_nn = ODEProblem(dudt_,u0, tspan, p)
sol_nn = concrete_solve(prob_nn, Tsit5(), u0, p, saveat = solution.t)
</code></pre>
<p>The thick curves represent the real solution, as we see, we get a decent predictor for only the first second, afterwards the prediction for $u_1(t)$ its pretty bad, while the prediction for $u_2(t)$ it&rsquo;s ok.</p>





  











<figure id="figure-bad-predictor">


  <a data-fancybox="" href="/img/post/SciML/Badpredictor.svg" data-caption="Bad predictor">


  <img src="/img/post/SciML/Badpredictor.svg" alt=""  >
</a>


  
  
  <figcaption>
    Bad predictor
  </figcaption>


</figure>

<p>Let&rsquo;s improve now. For this we will do as before, we perfom a prediction and then compute a <code>loss</code> function on the prediction to check how well are we fitting the data. Finally we create a <code>Callback</code> that saves the losses each $50$ iterations.</p>
<pre><code class="language-julia">function predict(Œ∏)
    Array(concrete_solve(prob_nn, Vern7(), u0, Œ∏, saveat = solution.t,
                         abstol=1e-6, reltol=1e-6,
                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))
end

function loss(Œ∏)
    pred = predict(Œ∏)
    sum(abs2, X‚Çô .- pred), pred 
end

const losses = []
callback(Œ∏,l,pred) = begin
    push!(losses, l)
    if length(losses)%50==0
        println(&quot;Current loss after $(length(losses)) iterations: $(losses[end])&quot;)
    end
    false
end
</code></pre>
<p>First train with <code>ADAM</code> for better convergence adn then train with <code>BFGS</code></p>
<pre><code class="language-julia">res1 = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 200)
res2 = DiffEqFlux.sciml_train(loss, res1.minimizer, BFGS(initial_stepnorm=0.01), 
                                                    cb=callback, 
                                                    maxiters = 10000)
</code></pre>





  











<figure id="figure-losses">


  <a data-fancybox="" href="/img/post/SciML/losses.svg" data-caption="losses">


  <img src="/img/post/SciML/losses.svg" alt=""  >
</a>


  
  
  <figcaption>
    losses
  </figcaption>


</figure>

<p>Final training loss after $482$ iterations is  $2.74 \times 10^{-4}$ and the approximation fits the real solution really well.</p>





  











<figure id="figure-real-solution-vs-approximation">


  <a data-fancybox="" href="/img/post/SciML/fitting.svg" data-caption="Real solution vs Approximation">


  <img src="/img/post/SciML/fitting.svg" alt=""  >
</a>


  
  
  <figcaption>
    Real solution vs Approximation
  </figcaption>


</figure>

<p>Notice that we purposely made the real solution curve thicker so that its easier to see, otherwise both curves are superposed.</p>
<p>We can also check the derivatives.</p>
<pre><code class="language-julia">DX = Array(solution(solution.t, Val{1}))
prob_nn2 = ODEProblem(dudt_,u0, tspan, res2.minimizer)
_sol = solve(prob_nn2, Tsit5())
DX_ = Array(_sol(solution.t, Val{1}))
</code></pre>





  











<figure id="figure-derivatives">


  <a data-fancybox="" href="/img/post/SciML/derivatives.svg" data-caption="Derivatives">


  <img src="/img/post/SciML/derivatives.svg" alt=""  >
</a>


  
  
  <figcaption>
    Derivatives
  </figcaption>


</figure>

<p>Finally, we know that the real functions are $\beta xy$ and $\gamma xy$. Lets check the error plot.</p>
<pre><code class="language-julia"># Ideal data
LÃÑ = [-p_[2]*(X[1,:].*X[2,:])';p_[3]*(X[1,:].*X[2,:])']
# Neural network guess
LÃÇ = L(X‚Çô,res2.minimizer)
</code></pre>





  











<figure id="figure-real-solution-vs-approximation">


  <a data-fancybox="" href="/img/post/SciML/error.svg" data-caption="Real solution vs Approximation">


  <img src="/img/post/SciML/error.svg" alt=""  >
</a>


  
  
  <figcaption>
    Real solution vs Approximation
  </figcaption>


</figure>

<h2 id="transforming-a-neural-network-fit-into-equations-in-sparsified-from-using-sindy">Transforming a neural network fit into equations in sparsified from using SInDy</h2>
<p>Now we want to use this nice fit and transformed back into equations. For this porpuse we&rsquo;ll use <code>ModelingToolkit.jl</code>.</p>
<p>We will let the model know that we have $2$ variables and then create a basis that can approximate this functions by linear combinations of $sin(u_1)$ , $cos(u_1)$, $sin(u_2)$, $cos(u_2)$, $constant$, $u_1(t)^k$, $u_2(t)^k$ and $u_1(t)^k * u_2(t)^{5-k}$ with $k = 1&hellip;5$</p>
<pre><code class="language-julia">@variables u[1:2]
# Lots of polynomials
polys = Operation[1]
for i ‚àà 1:5
    push!(polys, u[1]^i)
    push!(polys, u[2]^i)
    for j ‚àà i:5
        if i != j
            push!(polys, (u[1]^i)*(u[2]^j))
            push!(polys, u[2]^i*u[1]^i)
        end
    end
end
# And some other stuff
h = [cos.(u)...; sin.(u)...; polys...]
basis = Basis(h, u)
</code></pre>
<p>Now we will create an optimizer for the SINDY problem 
<a href="https://datadriven.sciml.ai/dev/sparse_identification/sindy/" target="_blank" rel="noopener">Check more about Sparse Identification of Nonlinear Dynamics</a></p>
<pre><code class="language-julia"># Create an optimizer for the SINDY problem
opt = SR3()
# Create the thresholds which should be used in the search process
Œª = exp10.(-7:0.1:3)
# Target function to choose the results from; x = L0 of coefficients and L2-Error of the model
f_target(x, w) = iszero(x[1]) ? Inf : norm(w.*x, 2)
</code></pre>
<p>Let&rsquo;s see what happens if we want to use pure SINDY, meaning we have no pior information, only the data generated by our neural network,i.e, we took the values of the differential equation through the time series, we run it on the neural network giving us the output $\hat{L}$ and the $X$ is the input of the neural network, then we make SINDY transform this data into a system of equations using the basis.</p>
<pre><code class="language-julia">Œ® = SInDy(X‚Çô[:, :], DX[:, :], basis, Œª, opt = opt, maxiter = 10000,
                                                f_target = f_target)
</code></pre>
<p>As a result we get : $$du_1 = \beta sin(u_1) + \alpha cos(u_2) + \gamma u_1^2$$
$$du_2 = \delta u_2 $$</p>
<p>i.e, we failed, but remember that we didn&rsquo;t use any pior information.</p>
<p>Test on ideal derivative data for unknown function (not available).</p>
<pre><code class="language-julia">Œ® = SInDy(X‚Çô[:, 5:end], LÃÑ[:, 5:end], basis, Œª, opt = opt, maxiter = 10000,
                                                        f_target = f_target)
</code></pre>
<p>This time we succeded as we recovered the missing terms of each equation.</p>
<p>$$du_1 = \beta u_1u_2 $$
$$du_2 = \gamma u_1u_2 $$</p>
<p>And we also succeded using derivative data.</p>
<pre><code class="language-julia"># Test on uode derivative data
println(&quot;SINDY on learned, partial, available data&quot;)
Œ® = SInDy(X‚Çô[:, 2:end], LÃÇ[:, 2:end], basis, Œª,  opt = opt, maxiter = 10000, 
                                                        normalize = true, 
                                                        denoise = true, 
                                                        f_target = f_target)
</code></pre>
<p>Now we can extract the parameters.</p>
<pre><code class="language-julia">pÃÇ = parameters(Œ®)
println(&quot;First parameter guess : $(pÃÇ)&quot;)
unknown_sys = ODESystem(Œ®)
unknown_eq = ODEFunction(unknown_sys)
</code></pre>
<p>The equations are recovered, but the parameters may not be the best, we can start another sindy run to get closer to the ground truth.</p>
<pre><code class="language-julia"># Just the equations
b = Basis((u, p, t)-&gt;unknown_eq(u, [1.; 1.], t), u)
# Retune for better parameters -&gt; we could also use DiffEqFlux or other parameter estimation tools here.
Œ®f = SInDy(X‚Çô[:, 2:end], LÃÇ[:, 2:end], b, opt = STRRidge(0.01), maxiter = 100, convergence_error = 1e-18) # Succeed
println(Œ®f)
pÃÇ = parameters(Œ®f)
println(&quot;Second parameter guess : $(pÃÇ)&quot;)
# Create function
recovered_sys = ODESystem(Œ®f)
recovered_eq = ODEFunction(recovered_sys)
# Build a ODE for the estimated system
function dudt(du, u, p, t)
    # Add SInDy Term
    Œ±, Œ¥, Œ≤, Œ≥ = p
    z = recovered_eq(u, [Œ≤; Œ≥], t)
    du[1] = Œ±*u[1] + z[1]
    du[2] = -Œ¥*u[2] + z[2]
end
# Create the approximated problem and solution
ps = [p_[[1,4]]; pÃÇ]
approximate_prob = ODEProblem(dudt, u0, tspan, ps)
approximate_solution = solve(approximate_prob, Tsit5(), saveat = 0.01)
# Look at long term prediction
t_long = (0.0, 50.0)
approximate_prob = ODEProblem(dudt, u0, t_long, ps)
approximate_solution_long = solve(approximate_prob, Tsit5(), saveat = 0.1) # Using higher tolerances here results in exit of julia
true_prob = ODEProblem(lotka, u0, t_long, p_)
true_solution_long = solve(true_prob, Tsit5(), saveat = approximate_solution_long.t)
</code></pre>
<p>Why are neural networks so good? because for large enough neural network, local optima are global optima.</p>
<p>
<a href="https://github.com/ChrisRackauckas/universal_differential_equations" target="_blank" rel="noopener">Click here to see more about Universal differential equations</a></p>
<h1 id="physics-informed-neural-networks">Physics-Informed Neural Networks</h1>
<h2 id="solve-and-ode-with-a-neural-network">Solve and ODE with a neural network</h2>
<ul>
<li>Let $u&rsquo; = f(u,t)$ with $u(0) = u_0$. We want to build a neural network $NN(t)$ that is the solution to this differential equation.</li>
<li>By definition then, we must have that $NN&rsquo;(t) = f(NN(t),t)$ and $NN(0) = u_0$.</li>
<li>Define $\mathcal{C}(\theta) = \displaystyle \sum_{t} \lVert NN&rsquo;(t) - f(NN(t),t) \rVert$ for $\theta$ the parameters of the ODE.
<ul>
<li>Then this cost is zero when $NN(t)$ is the solution to the ODE</li>
<li>Therefore we aim to minimize this loss to get the solution.</li>
</ul>
</li>
<li>Extra trick: We can use $g(t) = tNN(t) + u_0$ as a test function instead of $NN(t)$. Notice that it is an approximator that always satisfies the boundary condition.</li>
</ul>
<h3 id="example">Example</h3>
<p>Let&rsquo;s solve the ODE: $$u&rsquo;(t) = cos(2\pi t) = f(u,t)$$ with initial condition $$u(0) = 1$$</p>
<p>Following the steps above, we create a neural network, a function $g(t)$ which we derivate and compute the $l_2$ norm of the difference between $g&rsquo;(t)$ and $u&rsquo;(t)$. In order to do this, we define $\epsilon$ as small as the precision of a <em>Float32</em> allow us.</p>
<pre><code class="language-julia">using Flux
NNODE = Chain( x -&gt; [x],
            Dense(1,32,tanh),
            Dense(32,1),
            first)
NNODE(1.0)
g(t) = t*NNODE(t) + 1f0

using Statistics
œµ = sqrt(eps(Float32))
loss() = mean(abs2( ( ( g(t+œµ) - g(t) )/œµ ) - cos(2œÄ*t) ) for t in 0f0:0.01f0:1f0)
</code></pre>
<p>We create a callback and train the neural network using a descent algorithm.</p>
<pre><code class="language-julia">iter = 0
cb = function()
    global iter += 1
    if iter % 500 == 0
        display( loss() )
    end
end
opt = Flux.Descent(0.01) 
data = Iterators.repeated( (), 5000 )
Flux.train!(loss, Flux.params(NNODE), data, opt; cb= cb)
</code></pre>
<p>Now we can compare to the real solution:</p>
<p>$$ u(t) = \int u&rsquo;(t) dt = \int cos(2\pi t)dt = \dfrac{sin(2\pi t)}{2\pi} + constant$$</p>
<p>but $u(0) = 1$ therefore we get $u(t) = 1 + \dfrac{sin(2\pi t)}{2\pi}$</p>





  











<figure id="figure-true-solution-vs-neural-network">


  <a data-fancybox="" href="/img/post/SciML/PINN.svg" data-caption="True Solution vs Neural Network">


  <img src="/img/post/SciML/PINN.svg" alt=""  >
</a>


  
  
  <figcaption>
    True Solution vs Neural Network
  </figcaption>


</figure>

<p>Why Physics- Informed Neural Networks?</p>
<ul>
<li>$\mathcal{C}(\theta) = C_{pde}(\theta) + C_{boundary} + C_{data}(\theta)$ can nudge a model towards data.
<ul>
<li>Equivalent to regularizing the neural network by a scientific equation</li>
</ul>
</li>
<li>Can train fast continuous surrogates by making the neural netowrk parameter dependent.</li>
</ul>
<h3 id="example-solving-a-100-dimensional-hamilton-jacobi-bellman-equation">Example: Solving a 100 Dimensional Hamilton-Jacobi-Bellman Equation</h3>
<p>For this problem we will use <code>NeuralPDE.jl</code> (
<a href="https://github.com/SciML/NeuralPDE.jl" target="_blank" rel="noopener">Check more here</a>). Following this steps:</p>
<ul>
<li>Write the function and equation.</li>
<li>Make $\sigma^\top \nabla u (t,X)$ a neural network.</li>
<li>Solve the resulting SDEs and learn $\sigma^\top \nabla u$ via:
  <!-- $$l(\theta) = \displaystyle \mathbb{E} \left[ | g(X_{t_N}) - \hat{u}(\{ X_{t_n}{_{0 \le n \le N}}\} , \{ W_{t_n}_{0 \le n \le N}\} )|^2 \right] $$ -->
</li>
</ul>
<pre><code class="language-julia">using NeuralPDE
using Flux
using DifferentialEquations
using LinearAlgebra
d = 100 # number of dimensions
X0 = fill(0.0f0, d) # initial value of stochastic control process
tspan = (0.0f0, 1.0f0)
Œª = 1.0f0

g(X) = log(0.5f0 + 0.5f0 * sum(X.^2))
f(X,u,œÉ·µÄ‚àáu,p,t) = -Œª * sum(œÉ·µÄ‚àáu.^2)
Œº_f(X,p,t) = zero(X)  # Vector d x 1 Œª
œÉ_f(X,p,t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) # Matrix d x d
prob = TerminalPDEProblem(g, f, Œº_f, œÉ_f, X0, tspan)
hls = 10 + d # hidden layer size
opt = Flux.ADAM(0.01)  # optimizer
# sub-neural network approximating solutions at the desired point
u0 = Flux.Chain(Dense(d, hls, relu),
                Dense(hls, hls, relu),
                Dense(hls, 1))
# sub-neural network approximating the spatial gradients at time point
œÉ·µÄ‚àáu = Flux.Chain(Dense(d + 1, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, d))
pdealg = NNPDENS(u0, œÉ·µÄ‚àáu, opt=opt)
@time ans = solve(prob, pdealg, verbose=true, maxiters=100, trajectories=100,
                            alg=EM(), dt=1.2, pabstol=1f-2)
</code></pre>
<p>We can solve this high dimensional problem in only $22.623969$ seconds $523.95 M$ allocations: $36.683 GiB$, $17.95%$ gc time.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/juliacon-2020/">JuliaCon 2020</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://danpereda.github.io/post/scientificmachinelearning/&amp;text=Scientific%20Machine%20Learning%20on%20Julia" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://danpereda.github.io/post/scientificmachinelearning/&amp;t=Scientific%20Machine%20Learning%20on%20Julia" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Scientific%20Machine%20Learning%20on%20Julia&amp;body=https://danpereda.github.io/post/scientificmachinelearning/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://danpereda.github.io/post/scientificmachinelearning/&amp;title=Scientific%20Machine%20Learning%20on%20Julia" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Scientific%20Machine%20Learning%20on%20Julia%20https://danpereda.github.io/post/scientificmachinelearning/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://danpereda.github.io/post/scientificmachinelearning/&amp;title=Scientific%20Machine%20Learning%20on%20Julia" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/daniel-pereda/avatar_hu589823eb8db5ccba2d36240fb24d55c6_129432_270x270_fill_q90_lanczos_center.jpg" alt="Daniel Pereda">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://danpereda.github.io/">Daniel Pereda</a></h5>
        <h6 class="card-subtitle">PhD Student</h6>
        <p class="card-text">My research interests include optimization, game theory and operation research.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:%20dpereda@dim.uchile.cl" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/Danieeelph" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=KXafRHQAAAAJ&amp;hl=en&amp;authuser=1" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/danpereda" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/daniel-pereda-7b2a71158/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/test/">Learn Julia via epidemic modelling</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/julia.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/matlab.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/R.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
